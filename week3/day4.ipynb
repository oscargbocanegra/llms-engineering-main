{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_header",
   "metadata": {},
   "source": [
    "# Advanced LLM Inference with Hugging Face\n",
    "\n",
    "Welcome to this professional guide on running Large Language Models (LLMs) efficiently using the Hugging Face `transformers` library.\n",
    "\n",
    "In this notebook, we will explore:\n",
    "1.  **Environment Setup**: Installing necessary libraries and configuring authentication.\n",
    "2.  **Model Selection**: Choosing state-of-the-art open-source models.\n",
    "3.  **Quantization**: Understanding and applying 4-bit quantization to run large models on consumer hardware (like T4 GPUs).\n",
    "4.  **Inference**: Loading models and generating text.\n",
    "5.  **Modularization**: Building a robust function to switch between models easily.\n",
    "\n",
    "This guide is designed to provide a deep understanding of the *how* and *why* behind modern LLM inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we need to install the essential libraries:\n",
    "- `transformers`: The core library for loading and running models.\n",
    "- `torch`: PyTorch, the underlying deep learning framework.\n",
    "- `bitsandbytes`: A library required for 4-bit and 8-bit quantization.\n",
    "- `accelerate`: Helps manage model loading and inference across devices.\n",
    "- `sentencepiece`: A tokenizer required by some models like Llama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests torch bitsandbytes transformers sentencepiece accelerate\n",
    "# %pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auth_header",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "Accessing certain models (like Llama 3.1) requires a Hugging Face account and a valid API token. We will securely load this token from an environment file (`.env`) or use the `huggingface_hub` login utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auth_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path='/workspace/.env', override=True)\n",
    "hf_token = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "# Clean the token if necessary\n",
    "if hf_token and hf_token.startswith('Bearer '):\n",
    "    hf_token = hf_token.replace('Bearer ', '')\n",
    "if hf_token:\n",
    "    hf_token = hf_token.strip()\n",
    "\n",
    "print(f\"Token loaded: {'Yes' if hf_token else 'No'}\")\n",
    "\n",
    "# Log in to Hugging Face\n",
    "if hf_token:\n",
    "    login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models_header",
   "metadata": {},
   "source": [
    "## 2. Model Selection\n",
    "\n",
    "We will be working with a selection of high-performance models. Defining them in variables allows us to easily switch between them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model identifiers\n",
    "LLAMA = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "PHI4 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "GEMMA3 = \"google/gemma-3-1b-it\"\n",
    "QWEN = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quant_header",
   "metadata": {},
   "source": [
    "## 3. Quantization Theory & Configuration\n",
    "\n",
    "### What is Quantization?\n",
    "Quantization is the process of reducing the precision of the numbers used to represent a model's parameters (weights). \n",
    "- **Standard**: FP32 (32-bit floating point) or FP16 (16-bit).\n",
    "- **Quantized**: INT4 (4-bit integers).\n",
    "\n",
    "By reducing precision, we significantly lower the memory footprint, allowing us to run large models (like Llama 3.1 8B) on consumer GPUs with limited VRAM (e.g., 16GB or even less).\n",
    "\n",
    "### Configuration\n",
    "We use `BitsAndBytesConfig` to define how we want to load the model:\n",
    "- `load_in_4bit=True`: Enable 4-bit loading.\n",
    "- `bnb_4bit_quant_type=\"nf4\"`: Use \"NormalFloat4\", a data type optimized for normally distributed weights.\n",
    "- `bnb_4bit_use_double_quant=True`: Quantize the quantization constants themselves for extra savings.\n",
    "- `bnb_4bit_compute_dtype=torch.bfloat16`: Perform calculations in 16-bit precision for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quant_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cache_header",
   "metadata": {},
   "source": [
    "### Cache Management\n",
    "\n",
    "To keep our workspace organized, we will define specific directories for each model. This prevents models from filling up the default cache partition and allows for easier management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cache_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Hugging Face cache directory\n",
    "hf_cache_base = os.getenv('HF_HOME', '/root/.cache/huggingface')\n",
    "\n",
    "# Define specific cache directories for each model\n",
    "model_cache_llama_3_1 = os.path.join(hf_cache_base, 'models', 'llama_3_1_8b')\n",
    "model_cache_phi3 = os.path.join(hf_cache_base, 'models', 'phi_3_mini')\n",
    "model_cache_gemma_3 = os.path.join(hf_cache_base, 'models', 'gemma_3_4b')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(model_cache_llama_3_1, exist_ok=True)\n",
    "os.makedirs(model_cache_phi3, exist_ok=True)\n",
    "os.makedirs(model_cache_gemma_3, exist_ok=True)\n",
    "\n",
    "print(f\"Llama Cache: {model_cache_llama_3_1}\")\n",
    "print(f\"Phi-3 Cache: {model_cache_phi3}\")\n",
    "print(f\"Gemma Cache: {model_cache_gemma_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_llama_header",
   "metadata": {},
   "source": [
    "## 4. Loading the Model\n",
    "\n",
    "Now we load the **Llama 3.1** model using our quantization configuration. This might take a few minutes depending on your internet speed and whether the model is already cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_llama",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LLAMA = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=quant_config,\n",
    "    cache_dir=model_cache_llama_3_1\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model loaded successfully from: {model_cache_llama_3_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory_header",
   "metadata": {},
   "source": [
    "### Memory Footprint Analysis\n",
    "\n",
    "Let's verify the effectiveness of our quantization. A standard 8B model in FP16 would require approximately 16GB of VRAM. Let's see how much our 4-bit version uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_footprint = MODEL_LLAMA.get_memory_footprint() / 1e6\n",
    "print(f\"Memory footprint: {memory_footprint:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_header",
   "metadata": {},
   "source": [
    "## 5. Running Inference\n",
    "\n",
    "To generate text, we need to:\n",
    "1.  **Tokenize**: Convert our text prompt into numbers (tokens) the model understands.\n",
    "2.  **Generate**: Feed tokens into the model to predict the next tokens.\n",
    "3.  **Decode**: Convert the predicted tokens back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare the input\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about data scientists.\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 2. Generate output\n",
    "outputs = MODEL_LLAMA.generate(inputs, max_new_tokens=100)\n",
    "\n",
    "# 3. Decode and print\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory before moving on\n",
    "del inputs, outputs, MODEL_LLAMA, tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular_header",
   "metadata": {},
   "source": [
    "## 6. Modular Inference Function\n",
    "\n",
    "To efficiently test multiple models without rewriting code, we'll encapsulate the loading and generation logic into a single function. This function handles:\n",
    "- Tokenization\n",
    "- Model Loading (with quantization)\n",
    "- Streaming generation (printing text as it's generated)\n",
    "- Memory cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gen_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def generate(model_name, messages, cache_dir):\n",
    "    print(f\"\\n--- Loading {model_name} ---\")\n",
    "    \n",
    "    # Initialize Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Prepare Inputs\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    \n",
    "    # Load Model\n",
    "    loaded_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        device_map=\"auto\", \n",
    "        quantization_config=quant_config, \n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    \n",
    "    # Generate\n",
    "    print(\"\\n--- Response ---\")\n",
    "    outputs = loaded_model.generate(inputs, max_new_tokens=150, streamer=streamer)\n",
    "    \n",
    "    # Cleanup\n",
    "    del tokenizer, streamer, loaded_model, inputs, outputs\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n--- Memory Cleared ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_phi",
   "metadata": {},
   "source": [
    "### Testing Phi-3\n",
    "Let's test the `Phi-3` model using our new function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_phi",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in one sentence.\"}\n",
    "]\n",
    "\n",
    "generate(PHI4, messages, model_cache_phi3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_gemma",
   "metadata": {},
   "source": [
    "### Testing Gemma\n",
    "Now let's try Google's `Gemma` model. Note that some models might not support system prompts in the same way, so we adjust the message structure if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_gemma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemma sometimes prefers just user messages or has specific template requirements\n",
    "messages_gemma = [\n",
    "    {\"role\": \"user\", \"content\": \"Who is the father of Pinocchio?\"}\n",
    "]\n",
    "\n",
    "generate(GEMMA3, messages_gemma, model_cache_gemma_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have successfully:\n",
    "1.  Configured a professional LLM environment.\n",
    "2.  Understood and applied 4-bit quantization.\n",
    "3.  Loaded and ran inference on multiple state-of-the-art models.\n",
    "4.  Created a modular function for efficient testing.\n",
    "\n",
    "This foundation allows you to explore even larger models and more complex applications on standard hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
