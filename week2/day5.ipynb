{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "day5-intro",
   "metadata": {},
   "source": [
    "# Day 5 - Multi-modal AI Assistant\n",
    "\n",
    "We'll now bring together what we've learned to make an AI Assistant that can generate images using DALL-E 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "day5-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "day5-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# API keys\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "ollama_base_url = os.getenv('OLLAMA_BASE_URL')\n",
    "ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "ollama_model = os.getenv('OLLAMA_MODEL', 'qwen3-coder:480b-cloud')\n",
    "\n",
    "# Verify API keys\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key loaded: {openai_api_key[:8]}...\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if ollama_base_url:\n",
    "    print(f\"Ollama configured at: {ollama_base_url}\")\n",
    "    print(f\"Ollama Model is : {ollama_model}\")\n",
    "\n",
    "# Initialize Clients\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "ollama_client = OpenAI(\n",
    "    base_url=f\"{ollama_base_url}/v1\",\n",
    "    api_key=ollama_api_key\n",
    ")\n",
    "\n",
    "print(\"All clients initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "day5-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def artist(city):\n",
    "    print(f\"Generating image for {city}...\")\n",
    "    try:\n",
    "        image_response = openai_client.images.generate(\n",
    "                model=\"dall-e-3\",\n",
    "                prompt=f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style\",\n",
    "                size=\"1024x1024\",\n",
    "                n=1,\n",
    "                response_format=\"b64_json\",\n",
    "            )\n",
    "        image_base64 = image_response.data[0].b64_json\n",
    "        image_data = base64.b64decode(image_base64)\n",
    "        img = Image.open(BytesIO(image_data))\n",
    "        \n",
    "        # Save to file\n",
    "        filename = f\"{city}_vacation.png\"\n",
    "        img.save(filename)\n",
    "        return f\"I have generated an image of {city} and saved it to {filename}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating image: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "day5-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_function = {\n",
    "    \"name\": \"artist\",\n",
    "    \"description\": \"Generate an image of a city in a pop-art style.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city to generate an image for\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [{\"type\": \"function\", \"function\": artist_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "day5-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are a helpful assistant that can generate images of cities using the artist tool.\n",
    "If the user asks to see a city or for an image of a city, use the artist tool.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "day5-handle-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_calls(message):\n",
    "    responses = []\n",
    "    for tool_call in message.tool_calls:\n",
    "        if tool_call.function.name == \"artist\":\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "            city = arguments.get('city')\n",
    "            result = artist(city)\n",
    "            responses.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": result,\n",
    "                \"tool_call_id\": tool_call.id\n",
    "            })\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "day5-chat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # First call to LLM to check for tool calls\n",
    "    response = ollama_client.chat.completions.create(\n",
    "        model=ollama_model,\n",
    "        messages=messages,\n",
    "        tools=tools\n",
    "    )\n",
    "    \n",
    "    if response.choices[0].finish_reason == \"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        # Handle tool calls\n",
    "        responses = handle_tool_calls(message)\n",
    "        \n",
    "        # Append assistant's tool call message and tool responses to history\n",
    "        messages.append(message)\n",
    "        messages.extend(responses)\n",
    "        \n",
    "        # Second call to LLM to get final response\n",
    "        stream = ollama_client.chat.completions.create(\n",
    "            model=ollama_model,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        response_content = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk.choices[0].delta.content or \"\"\n",
    "            response_content += content\n",
    "            yield response_content\n",
    "            \n",
    "    else:\n",
    "        # No tool call, just return the response\n",
    "        yield response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "day5-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
