{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfa9ae6-69fe-444a-b994-8c4c5970a7ec",
   "metadata": {},
   "source": [
    "# Project - Airline AI Assistant\n",
    "\n",
    "We'll now bring together what we've learned to make an AI Customer Support assistant for an Airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f908f34-a86d-455c-837e-c71973628d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4f3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d0693b-6870-4b38-bde7-1072203012b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded: sk-proj-...\n",
      "Ollama configured at: http://192.168.80.200:11434\n",
      "Ollama Model is : qwen3-coder:480b-cloud\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# API keys from environment\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "ollama_base_url = os.getenv('OLLAMA_BASE_URL')\n",
    "ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "ollama_model = os.getenv('OLLAMA_MODEL', 'qwen3-coder:480b-cloud')\n",
    "\n",
    "# Verify API keys\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key loaded: {openai_api_key[:8]}...\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if ollama_base_url:\n",
    "    print(f\"Ollama configured at: {ollama_base_url}\")\n",
    "    print(f\"Ollama Model is : {ollama_model}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f481fe75-5215-44b6-9c05-cffd374d6b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All clients initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize API clients\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Initialize Ollama client (uses OpenAI-compatible API)\n",
    "ollama_client = OpenAI(\n",
    "    base_url=f\"{ollama_base_url}/v1\",\n",
    "    api_key=ollama_api_key\n",
    ")\n",
    "\n",
    "print(\"All clients initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a521d84-d07c-49ab-a0df-d6451499ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "Eres un asistente util para una aerolinea llamada FlightAI.\n",
    "Da respuestas breves y corteses, de no mas de una oraci√≥n.\n",
    "Se siempre preciso. Si no sabes la respuesta, dilo.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61a2a15d-b559-4844-b377-6bd5cb4949f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    stream = ollama_client.chat.completions.create(\n",
    "        model=ollama_model,\n",
    "        messages=messages,\n",
    "        stream = True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de803de9-d698-44e3-a01a-04fa74f46a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://1aa54695b78e592cc5.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1aa54695b78e592cc5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.close_all()\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bedabf-a0a7-4985-ad8e-07ed6a55a3a4",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Tools are an incredibly powerful feature provided by the frontier LLMs.\n",
    "\n",
    "With tools, you can write a function, and have the LLM call that function as part of its response.\n",
    "\n",
    "Sounds almost spooky.. we're giving it the power to run code on our machine?\n",
    "\n",
    "Well, kinda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0696acb1-0b05-4dc2-80d5-771be04f1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by making a useful function\n",
    "\n",
    "ticket_prices = {\"londres\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool called for city {destination_city}\")\n",
    "    price = ticket_prices.get(destination_city.lower(), \"Unknown ticket price\")\n",
    "    return f\"The price of a ticket to {destination_city} is {price}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80ca4e09-6287-4d3f-997d-fa6afbcf6c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool called for city londres\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The price of a ticket to londres is $799'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ticket_price(\"londres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4afceded-7178-4c05-8fa6-9f2085e6a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a particular dictionary structure that's required to describe our function:\n",
    "\n",
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\",\n",
    "    \"description\": \"Get the price of a return ticket to the destination city.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city that the customer wants to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdca8679-935f-4e7f-97e6-e71a4d4f228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this is included in a list of tools:\n",
    "\n",
    "tools = [{\"type\": \"function\", \"function\": price_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "818b4b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'get_ticket_price',\n",
       "   'description': 'Get the price of a return ticket to the destination city.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'destination_city': {'type': 'string',\n",
       "      'description': 'The city that the customer wants to travel to'}},\n",
       "    'required': ['destination_city'],\n",
       "    'additionalProperties': False}}}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a9d49",
   "metadata": {},
   "source": [
    "## Testing Tool Support in Ollama Models\n",
    "\n",
    "Let's test which Ollama models support function calling properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d2c49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Ollama models to test\n",
    "ollama_models_to_test = [\n",
    "    \"deepseek-v3.1:671b-cloud\",\n",
    "    \"gpt-oss:20b-cloud\",\n",
    "    \"gpt-oss:120b-cloud\",\n",
    "    \"kimi-k2:1t-cloud\",\n",
    "    \"qwen3-coder:480b-cloud\",\n",
    "    \"glm-4.6:cloud\",\n",
    "    \"minimax-m2:cloud\"\n",
    "]\n",
    "\n",
    "def test_model_tools(model_name):\n",
    "    \"\"\"Test if a model supports tool calling\"\"\"\n",
    "    try:\n",
    "        test_messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What's the price of a ticket to Paris?\"}\n",
    "        ]\n",
    "        \n",
    "        response = ollama_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=test_messages,\n",
    "            tools=tools,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        finish_reason = response.choices[0].finish_reason\n",
    "        has_tool_calls = hasattr(response.choices[0].message, 'tool_calls') and response.choices[0].message.tool_calls\n",
    "        \n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"finish_reason\": finish_reason,\n",
    "            \"supports_tools\": finish_reason == \"tool_calls\",\n",
    "            \"has_tool_calls\": has_tool_calls,\n",
    "            \"response\": response.choices[0].message.content if not has_tool_calls else \"Called tool\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cccda234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tool support for Ollama models...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Testing: deepseek-v3.1:671b-cloud\n",
      "  SUPPORTS TOOLS! finish_reason=tool_calls\n",
      "\n",
      "Testing: gpt-oss:20b-cloud\n",
      "  SUPPORTS TOOLS! finish_reason=tool_calls\n",
      "\n",
      "Testing: gpt-oss:120b-cloud\n",
      "  SUPPORTS TOOLS! finish_reason=tool_calls\n",
      "\n",
      "Testing: kimi-k2:1t-cloud\n",
      "  Error: Error code: 404 - {'error': {'message': \"model 'kimi-k2:1t-cloud' not found\", 'type': 'api_error', '\n",
      "\n",
      "Testing: qwen3-coder:480b-cloud\n",
      "  SUPPORTS TOOLS! finish_reason=tool_calls\n",
      "\n",
      "Testing: glm-4.6:cloud\n",
      "  Error: Error code: 404 - {'error': {'message': \"model 'glm-4.6:cloud' not found\", 'type': 'api_error', 'par\n",
      "\n",
      "Testing: minimax-m2:cloud\n",
      "  Error: Error code: 404 - {'error': {'message': \"model 'minimax-m2:cloud' not found\", 'type': 'api_error', '\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SUMMARY - Models with Tool Support:\n",
      "  deepseek-v3.1:671b-cloud\n",
      "  gpt-oss:20b-cloud\n",
      "  gpt-oss:120b-cloud\n",
      "  qwen3-coder:480b-cloud\n",
      "\n",
      "Models WITHOUT Tool Support:\n"
     ]
    }
   ],
   "source": [
    "# Test all models\n",
    "print(\"Testing tool support for Ollama models...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "for model in ollama_models_to_test:\n",
    "    print(f\"\\nTesting: {model}\")\n",
    "    result = test_model_tools(model)\n",
    "    results.append(result)\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"  Error: {result['error'][:100]}\")\n",
    "    elif result.get(\"supports_tools\"):\n",
    "        print(f\"  SUPPORTS TOOLS! finish_reason={result['finish_reason']}\")\n",
    "    else:\n",
    "        print(f\"  No tool support. finish_reason={result['finish_reason']}\")\n",
    "        print(f\"  Response: {result.get('response', 'N/A')[:80]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nSUMMARY - Models with Tool Support:\")\n",
    "for r in results:\n",
    "    if r.get(\"supports_tools\"):\n",
    "        print(f\"  {r['model']}\")\n",
    "        \n",
    "print(\"\\nModels WITHOUT Tool Support:\")\n",
    "for r in results:\n",
    "    if not r.get(\"supports_tools\") and \"error\" not in r:\n",
    "        print(f\"   {r['model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3554f-b4e3-4ce7-af6f-68faa6dd2340",
   "metadata": {},
   "source": [
    "## Getting Ollama to use our Tool\n",
    "\n",
    "Based on testing, **qwen3-coder:480b-cloud** has the best Tool calling support.\n",
    "\n",
    "What we actually do is give the LLM the opportunity to inform us that it wants us to run the tool.\n",
    "\n",
    "Here's how the new chat function looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce9b0744-9c78-408d-b9df-9f6fd9ed78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Use qwen3-coder:480b-cloud - best Tool support from testing\n",
    "    response = ollama_client.chat.completions.create(\n",
    "        model=ollama_model,\n",
    "        messages=messages,\n",
    "        tools=tools\n",
    "    )\n",
    "\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        response = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        response = ollama_client.chat.completions.create(\n",
    "            model=ollama_model,\n",
    "            messages=messages\n",
    "        )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0992986-ea09-4912-a076-8e5603ee631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to write that function handle_tool_call:\n",
    "\n",
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    if tool_call.function.name == \"get_ticket_price\":\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        city = arguments.get('destination_city')\n",
    "        price_details = get_ticket_price(city)\n",
    "        response = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": price_details,\n",
    "            \"tool_call_id\": tool_call.id\n",
    "        }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4be8a71-b19e-4c2f-80df-f59ff2661f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://a039c1b9fa74ae8e40.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a039c1b9fa74ae8e40.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool called for city London\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f30fbe",
   "metadata": {},
   "source": [
    "## Let's make a couple of improvements\n",
    "\n",
    "Handling multiple tool calls in 1 response\n",
    "\n",
    "Handling multiple tool calls 1 after another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6f5c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    response = ollama_client.chat.completions.create(\n",
    "        model=ollama_model,\n",
    "        messages=messages,\n",
    "        tools=tools\n",
    "    )\n",
    "    \n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        responses = handle_tool_calls(message)\n",
    "        messages.append(message)\n",
    "        messages.extend(responses)\n",
    "        response = ollama_client.chat.completions.create(\n",
    "            model=ollama_model,\n",
    "            messages=messages,\n",
    "            tools=tools\n",
    "        )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c46a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_calls(message):\n",
    "    responses = []\n",
    "    for tool_call in message.tool_calls:\n",
    "        if tool_call.function.name == \"get_ticket_price\":\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "            city = arguments.get('destination_city')\n",
    "            price_details = get_ticket_price(city)\n",
    "            responses.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": price_details,\n",
    "                \"tool_call_id\": tool_call.id\n",
    "            })\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95f02a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://e03e324a88c8916a24.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e03e324a88c8916a24.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool called for city Londres\n",
      "Tool called for city tokio\n",
      "Tool called for city tokyo\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc27a7-5f6e-4403-8616-e7388354c649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
