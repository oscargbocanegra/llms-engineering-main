{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a8098a",
   "metadata": {},
   "source": [
    "1. Visión del proyecto (para explicarlo en clase/entrevista)\n",
    "\n",
    "“Construí un generador de datos sintéticos que usa modelos de lenguaje open-source (Llama 3.1 / 3.2 y Gemma 2) para crear datasets tabulares a partir de descripciones de negocio. El usuario define el dominio (ej. ventas, banca, salud), el tamaño del dataset y las reglas; el modelo genera datos en formato CSV. Encima de esto construí una interfaz con Gradio para que cualquier usuario pueda usarlo sin escribir código.”\n",
    "\n",
    "Casos de uso a mencionar:\n",
    "\n",
    "- Probar pipelines de datos cuando no hay datos reales disponibles.\n",
    "- Crear datos despersonalizados para demos / prototipos.\n",
    "- Augmentación de datos para ejercicios de ML.\n",
    "\n",
    "2. Tecnologías a usar\n",
    "\n",
    "- Modelos LLM (texto-texto):\n",
    "- meta-llama/Llama-3.1-8B-Instruct \n",
    "- meta-llama/Llama-3.2-3B-Instruct \n",
    "- google/gemma-2-9b-it \n",
    "\n",
    "Librerías Python:\n",
    "\n",
    "- transformers o huggingface_hub (para llamar al modelo).\n",
    "- pandas (para construir el DataFrame).\n",
    "\n",
    "gradio (UI).\n",
    "\n",
    "- Auth Hugging Face con token. \n",
    "\n",
    "3. Estructura sugerida del notebook Project3-week3.ipynb\n",
    "\n",
    "- Título + descripción del proyecto (Markdown).\n",
    "- Instalación / imports.\n",
    "- Configuración de modelos Hugging Face.\n",
    "- Definición de esquemas de datasets (plantillas de negocio).\n",
    "- Función generadora usando LLM.\n",
    "- Conversión a pandas.DataFrame + validaciones básicas.\n",
    "- Interfaz Gradio.\n",
    "\n",
    "Pruebas y ejemplos de uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89022227",
   "metadata": {},
   "source": [
    "### Imports e instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub gradio pandas BitsAndBytesConfig dotenv openai torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb487d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from huggingface_hub import InferenceClient, login\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148d20b",
   "metadata": {},
   "source": [
    "### Login Hugging face and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(dotenv_path='/workspace/.env', override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "hf_token = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "if openai_api_key:\n",
    "    openai_client = OpenAI(api_key=openai_api_key)\n",
    "    print(\" OpenAI Client Initialized\")\n",
    "else:\n",
    "    print(\" OpenAI API Key not found\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "if hf_token:\n",
    "    if hf_token.startswith('Bearer '):\n",
    "        hf_token = hf_token.replace('Bearer ', '')\n",
    "    login(hf_token.strip())\n",
    "    print(\" Logged into Hugging Face\")\n",
    "else:\n",
    "    print(\" Hugging Face Token not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3dcee",
   "metadata": {},
   "source": [
    "### Configuración de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model identifiers\n",
    "LLAMA_3_1 = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "LLAMA_3_2 = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "PHI4 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "GEMMA3 = \"google/gemma-3-1b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864bcff",
   "metadata": {},
   "source": [
    "### Cache Management\n",
    "\n",
    "To keep our workspace organized, we will define specific directories for each model. This prevents models from filling up the default cache partition and allows for easier management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Hugging Face cache directory\n",
    "hf_cache_base = os.getenv('HF_HOME', '/root/.cache/huggingface')\n",
    "\n",
    "# Define specific cache directories for each model\n",
    "model_cache_llama = os.path.join(hf_cache_base, 'models', 'llama_3_1_8b')\n",
    "model_cache_llama = os.path.join(hf_cache_base, 'models', 'llama_3_2_8b')\n",
    "model_cache_phi = os.path.join(hf_cache_base, 'models', 'phi_3_mini')\n",
    "model_cache_gemma = os.path.join(hf_cache_base, 'models', 'gemma_3_4b')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(model_cache_llama, exist_ok=True)\n",
    "os.makedirs(model_cache_phi = os.path.join(hf_cache_base, 'models', 'phi_3_mini')\n",
    ", exist_ok=True)\n",
    "os.makedirs(model_cache_gemma, exist_ok=True)\n",
    "\n",
    "print(f\"Llama Cache: {model_cache_llama}\")\n",
    "print(f\"Phi-3 Cache: {model_cache_phi}\")\n",
    "print(f\"Gemma Cache: {model_cache_gemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39476b",
   "metadata": {},
   "source": [
    "### Quantization Theory & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
