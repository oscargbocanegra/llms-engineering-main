{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a8098a",
   "metadata": {},
   "source": [
    "1. Visi√≥n del proyecto (para explicarlo en clase/entrevista)\n",
    "\n",
    "‚ÄúConstru√≠ un generador de datos sint√©ticos que usa modelos de lenguaje open-source (Llama 3.1 / 3.2 y Gemma 2) para crear datasets tabulares a partir de descripciones de negocio. El usuario define el dominio (ej. ventas, banca, salud), el tama√±o del dataset y las reglas; el modelo genera datos en formato CSV. Encima de esto constru√≠ una interfaz con Gradio para que cualquier usuario pueda usarlo sin escribir c√≥digo.‚Äù\n",
    "\n",
    "Casos de uso a mencionar:\n",
    "\n",
    "- Probar pipelines de datos cuando no hay datos reales disponibles.\n",
    "- Crear datos despersonalizados para demos / prototipos.\n",
    "- Augmentaci√≥n de datos para ejercicios de ML.\n",
    "\n",
    "2. Tecnolog√≠as a usar\n",
    "\n",
    "- Modelos LLM (texto-texto):\n",
    "- meta-llama/Llama-3.1-8B-Instruct \n",
    "- meta-llama/Llama-3.2-3B-Instruct \n",
    "- google/gemma-2-9b-it \n",
    "\n",
    "Librer√≠as Python:\n",
    "\n",
    "- transformers o huggingface_hub (para llamar al modelo).\n",
    "- pandas (para construir el DataFrame).\n",
    "\n",
    "gradio (UI).\n",
    "\n",
    "- Auth Hugging Face con token. \n",
    "\n",
    "3. Estructura sugerida del notebook Project3-week3.ipynb\n",
    "\n",
    "- T√≠tulo + descripci√≥n del proyecto (Markdown).\n",
    "- Instalaci√≥n / imports.\n",
    "- Configuraci√≥n de modelos Hugging Face.\n",
    "- Definici√≥n de esquemas de datasets (plantillas de negocio).\n",
    "- Funci√≥n generadora usando LLM.\n",
    "- Conversi√≥n a pandas.DataFrame + validaciones b√°sicas.\n",
    "- Interfaz Gradio.\n",
    "\n",
    "Pruebas y ejemplos de uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89022227",
   "metadata": {},
   "source": [
    "## Imports e instalaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub gradio pandas bitsandbytes dotenv openai torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb487d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/LLM/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148d20b",
   "metadata": {},
   "source": [
    "## Login y carga de variables de entorno\n",
    "- Carga .env.\n",
    "- Toma OPENAI_API_KEY y HUGGINGFACE_API_KEY.\n",
    "- Crea un cliente de OpenAI si hay API key.\n",
    "- Llama a login() de Hugging Face si hay token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd3696e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OpenAI Client Initialized\n",
      " Logged into Hugging Face\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(dotenv_path='/workspace/.env', override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "hf_token = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "if openai_api_key:\n",
    "    openai_client = OpenAI(api_key=openai_api_key)\n",
    "    print(\" OpenAI Client Initialized\")\n",
    "else:\n",
    "    print(\" OpenAI API Key not found\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "if hf_token:\n",
    "    if hf_token.startswith('Bearer '):\n",
    "        hf_token = hf_token.replace('Bearer ', '')\n",
    "    login(hf_token.strip())\n",
    "    print(\" Logged into Hugging Face\")\n",
    "else:\n",
    "    print(\" Hugging Face Token not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3dcee",
   "metadata": {},
   "source": [
    "## Configuraci√≥n de modelos\n",
    "- Definision de nombres de modelos en Hugging Face.\n",
    "- En este proyecto se est√° usando LLAMA_3_1 en 4 bits; los dem√°s est√°n listos por si se usan luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af9eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model identifiers\n",
    "LLAMA_3_1 = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "LLAMA_3_2 = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "PHI4 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "GEMMA3 = \"google/gemma-3-4b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864bcff",
   "metadata": {},
   "source": [
    "## Cache Management\n",
    "\n",
    "To keep our workspace organized, we will define specific directories for each model. This prevents models from filling up the default cache partition and allows for easier management.\n",
    "\n",
    "- Define ruta base de cach√© HF.\n",
    "- Crea subdirectorios para cada modelo.\n",
    "- Imprime rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f6432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama Cache: /root/.cache/huggingface/models/llama_3_1_8b\n",
      "Llama Cache: /root/.cache/huggingface/models/llama_3_2_3b\n",
      "Phi-3 Cache: /root/.cache/huggingface/models/phi_3_mini\n",
      "Gemma Cache: /root/.cache/huggingface/models/gemma_3_4b\n"
     ]
    }
   ],
   "source": [
    "# Base Hugging Face cache directory\n",
    "hf_cache_base = os.getenv('HF_HOME', '/root/.cache/huggingface')\n",
    "\n",
    "# Define specific cache directories for each model\n",
    "model_cache_llama_3_1_8b = os.path.join(hf_cache_base, 'models', 'llama_3_1_8b')\n",
    "model_cache_llama_3_2_3b = os.path.join(hf_cache_base, 'models', 'llama_3_2_3b')\n",
    "model_cache_phi = os.path.join(hf_cache_base, 'models', 'phi_3_mini')\n",
    "model_cache_gemma = os.path.join(hf_cache_base, 'models', 'gemma_3_4b')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(model_cache_llama_3_1_8b, exist_ok=True)\n",
    "os.makedirs(model_cache_llama_3_2_3b, exist_ok=True)\n",
    "os.makedirs(model_cache_phi, exist_ok=True)\n",
    "os.makedirs(model_cache_gemma, exist_ok=True)\n",
    "\n",
    "print(f\"Llama Cache: {model_cache_llama_3_1_8b}\")\n",
    "print(f\"Llama Cache: {model_cache_llama_3_2_3b}\")\n",
    "print(f\"Phi-3 Cache: {model_cache_phi}\")\n",
    "print(f\"Gemma Cache: {model_cache_gemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d54ddf-f51c-41a5-8567-5cf411bd1eb9",
   "metadata": {},
   "source": [
    "## Definir ‚Äúplantillas de datasets‚Äù\n",
    "3 dominios de negocio:\n",
    "- \"Retail Sales\"\n",
    "- \"Bank Transactions\"\n",
    "- \"Customer Support Tickets\"\n",
    "\n",
    "Cada uno tiene:\n",
    "\n",
    "- description\n",
    "- columns: lista de columnas con name, type, constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11eed930-5c55-45af-b549-b7713488255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SCHEMAS = {\n",
    "    \"Retail Sales\": {\n",
    "        \"description\": \"Ventas en una tienda retail de e-commerce.\",\n",
    "        \"columns\": [\n",
    "            {\"name\": \"order_id\", \"type\": \"string\", \"constraints\": \"√∫nico, formato ORD-XXXX\"},\n",
    "            {\"name\": \"order_date\", \"type\": \"date\", \"constraints\": \"entre 2024-01-01 y 2024-12-31\"},\n",
    "            {\"name\": \"customer_id\", \"type\": \"string\", \"constraints\": \"formato CUST-XXXX\"},\n",
    "            {\"name\": \"country\", \"type\": \"category\", \"constraints\": \"Colombia, M√©xico, Chile, Per√∫\"},\n",
    "            {\"name\": \"product_category\", \"type\": \"category\", \"constraints\": \"Electr√≥nicos, Ropa, Hogar\"},\n",
    "            {\"name\": \"unit_price\", \"type\": \"float\", \"constraints\": \"entre 5 y 200\"},\n",
    "            {\"name\": \"quantity\", \"type\": \"int\", \"constraints\": \"entre 1 y 10\"},\n",
    "            {\"name\": \"total_amount\", \"type\": \"float\", \"constraints\": \"unit_price * quantity\"},\n",
    "            {\"name\": \"is_fraud\", \"type\": \"bool\", \"constraints\": \"True si la transacci√≥n es fraudulenta, False en caso contrario\"}\n",
    "        ]\n",
    "    },\n",
    "    \"Bank Transactions\": {\n",
    "        \"description\": \"Movimientos bancarios de cuentas de ahorro.\",\n",
    "        \"columns\": [\n",
    "            {\"name\": \"transaction_id\", \"type\": \"string\", \"constraints\": \"√∫nico\"},\n",
    "            {\"name\": \"customer_id\", \"type\": \"string\", \"constraints\": \"formato CUST-XXXX\"},\n",
    "            {\"name\": \"transaction_date\", \"type\": \"date\", \"constraints\": \"2024-01-01 a 2024-12-31\"},\n",
    "            {\"name\": \"transaction_type\", \"type\": \"category\", \"constraints\": \"deposit, withdrawal, transfer\"},\n",
    "            {\"name\": \"amount\", \"type\": \"float\", \"constraints\": \"entre 10 y 5_000\"},\n",
    "            {\"name\": \"balance_after\", \"type\": \"float\", \"constraints\": \"saldo posterior coherente\"},\n",
    "            {\"name\": \"channel\", \"type\": \"category\", \"constraints\": \"ATM, web, mobile_app, branch\"}\n",
    "        ]\n",
    "    },\n",
    "    \"Customer Support Tickets\": {\n",
    "        \"description\": \"Tickets de soporte para una plataforma SaaS.\",\n",
    "        \"columns\": [\n",
    "            {\"name\": \"ticket_id\", \"type\": \"string\", \"constraints\": \"√∫nico\"},\n",
    "            {\"name\": \"created_at\", \"type\": \"datetime\", \"constraints\": \"2024-01-01 a 2024-12-31\"},\n",
    "            {\"name\": \"customer_tier\", \"type\": \"category\", \"constraints\": \"Free, Standard, Premium\"},\n",
    "            {\"name\": \"issue_type\", \"type\": \"category\", \"constraints\": \"bug, billing, onboarding, other\"},\n",
    "            {\"name\": \"priority\", \"type\": \"category\", \"constraints\": \"low, medium, high, critical\"},\n",
    "            {\"name\": \"resolution_time_hours\", \"type\": \"float\", \"constraints\": \">= 0\"},\n",
    "            {\"name\": \"resolved\", \"type\": \"bool\", \"constraints\": \"True/False\"}\n",
    "        ]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a130b31-1136-46ff-8023-3a415363cf60",
   "metadata": {},
   "source": [
    "## Construir el prompt para el LLM\n",
    "### Construye el prompt en espa√±ol explic√°ndole al LLM:\n",
    "- Rol (generador de datos sint√©ticos).\n",
    "- Dataset y descripci√≥n.\n",
    "- Columnas y restricciones.\n",
    "- N√∫mero de filas.\n",
    "- Reglas de salida: solo CSV, encabezado en primera fila, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7033b1dc-7718-4386-9fd6-68f6b1bca73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(schema_name: str, n_rows: int, extra_instructions: str = \"\") -> str:\n",
    "    schema = DATASET_SCHEMAS[schema_name]\n",
    "    lines = []\n",
    "\n",
    "    lines.append(\n",
    "        \"Eres un generador de datos sint√©ticos tabulares para pruebas de anal√≠tica y machine learning.\"\n",
    "    )\n",
    "    lines.append(\n",
    "        \"Tu tarea es generar un dataset SINT√âTICO en formato CSV, sin datos personales reales.\"\n",
    "    )\n",
    "    lines.append(f\"Dataset: {schema_name}\")\n",
    "    lines.append(f\"Descripci√≥n: {schema['description']}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Especificaci√≥n de columnas:\")\n",
    "\n",
    "    for col in schema[\"columns\"]:\n",
    "        lines.append(\n",
    "            f\"- {col['name']} ({col['type']}): {col['constraints']}\"\n",
    "        )\n",
    "\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"Genera exactamente {n_rows} filas de datos. Es obligatorio tener {n_rows} filas.\")\n",
    "    lines.append(\"No escribas texto adicional antes ni despu√©s del CSV. Solo el CSV.\")\n",
    "    lines.append(\"Muy importante:\")\n",
    "    lines.append(\"1. La salida debe estar SOLO en formato CSV.\")\n",
    "    lines.append(\"2. La primera fila debe ser el encabezado con los nombres de las columnas.\")\n",
    "    lines.append(\"3. No incluyas explicaciones, comentarios ni texto adicional.\")\n",
    "    lines.append(\"4. Respeta tipos y rangos lo mejor posible.\")\n",
    "    if extra_instructions:\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"Instrucciones adicionales del usuario:\")\n",
    "        lines.append(extra_instructions)\n",
    "\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5daa7b-602d-483c-b648-d7f67000a8ae",
   "metadata": {},
   "source": [
    "## Cargar el tokenizer de Llama 3.1\n",
    "- Descarga / carga el tokenizer de Llama 3.1 8B.\n",
    "### Ajusta:\n",
    "- pad_token = eos_token\n",
    "- padding_side = \"left\" (√∫til para modelos causales).\n",
    "- Aqu√≠ se define tokenizer_llama, que luego usa generate_with_local_llama.\n",
    "- Si esta celda no se ejecuta antes de pulsar el bot√≥n de Gradio ‚Üí NameError: tokenizer_llama is not defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba62c42-ff6a-4f99-9e79-3c0199b00977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "tokenizer_llama = AutoTokenizer.from_pretrained(\n",
    "    LLAMA_3_1,\n",
    "    cache_dir=model_cache_llama_3_1_8b\n",
    ")\n",
    "\n",
    "# Ajustes recomendados para modelos causales\n",
    "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
    "tokenizer_llama.padding_side = \"left\"\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39476b",
   "metadata": {},
   "source": [
    "## Configura  --> Quantization\n",
    "### Configura BitsAndBytes para cargar el modelo:\n",
    "- En 4 bits (load_in_4bit=True)\n",
    "- NF4 como tipo de cuantizaci√≥n.\n",
    "- bfloat16 como tipo de c√≥mputo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6252ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181ad96-30f0-45be-a8a8-9ef07bbba70b",
   "metadata": {},
   "source": [
    "### Carga del modelo Llama 3.1 en 4-bit\n",
    "- Carga el modelo en 4-bit usando quant_config, mapeando dispositivos autom√°ticamente.\n",
    "- Usa el mismo cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6b0b04a-1bc4-4757-9893-66e72d45afde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db4fab261244ff982e10c2d21dfc6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully from: /root/.cache/huggingface/models/llama_3_1_8b\n"
     ]
    }
   ],
   "source": [
    "MODEL_LLAMA = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_3_1, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=quant_config,\n",
    "    cache_dir=model_cache_llama_3_1_8b\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully from: {model_cache_llama_3_1_8b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e6a493-e66c-459a-b097-8ac035e1fb16",
   "metadata": {},
   "source": [
    "## Funci√≥n generate_with_local_llama con limpieza de tensores\n",
    "- Usa tokenizer_llama y MODEL_LLAMA\n",
    "- Genera texto con sampling (temperature, top_p).\n",
    "- Limpia memoria: del inputs, generated_ids + torch.cuda.empty_cache()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d67db43-d549-474e-a935-001373ca293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_local_llama(prompt: str, max_new_tokens: int = 4096) -> str:\n",
    "    # Tokenizamos el prompt\n",
    "    inputs = tokenizer_llama(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(MODEL_LLAMA.device)\n",
    "\n",
    "    # Generaci√≥n\n",
    "    with torch.no_grad():\n",
    "        generated_ids = MODEL_LLAMA.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_llama.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decodificar el texto completo (prompt + respuesta)\n",
    "    full_text = tokenizer_llama.decode(\n",
    "        generated_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # Limpiar tensores intermedios para liberar memoria GPU\n",
    "    del inputs, generated_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Opcional: quedarte solo con lo generado despu√©s del prompt\n",
    "    generated_part = full_text[len(prompt):].strip()\n",
    "    return generated_part if generated_part else full_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333920c9-b787-477f-9a53-752ecbcf61ba",
   "metadata": {},
   "source": [
    "### Parsear la salida a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5478fc3-ec3b-4fde-b8ab-9b85c8a91826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def parse_csv_to_df(text: str) -> pd.DataFrame:\n",
    "    # Quitar posibles bloques markdown ```csv ... ```\n",
    "    cleaned = re.sub(r\"```(?:csv)?\", \"\", text)\n",
    "    cleaned = cleaned.strip(\"` \\n\")\n",
    "\n",
    "    # Nos quedamos solo con las l√≠neas que contengan comas (probable CSV)\n",
    "    lines = [l for l in cleaned.splitlines() if \",\" in l]\n",
    "    if not lines:\n",
    "        print(\"WARN: No se encontraron l√≠neas con comas en la salida del modelo.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    csv_text = \"\\n\".join(lines)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(io.StringIO(csv_text))\n",
    "    except Exception as e:\n",
    "        print(\"Error al parsear CSV:\", e)\n",
    "        print(\"Contenido que se intent√≥ parsear:\")\n",
    "        print(csv_text[:500])\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d2272-e647-4702-884f-721bc2abcafe",
   "metadata": {},
   "source": [
    "### Validaciones b√°sicas\n",
    "- ¬øN√∫mero de filas = solicitado?\n",
    "- ¬øHay columnas faltantes?\n",
    "- ¬øTipos b√°sicos (int, float) se pueden convertir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9a849db-02fe-4b1f-a696-ea19cf070d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_quality_checks(df: pd.DataFrame, schema_name: str) -> dict:\n",
    "    schema = DATASET_SCHEMAS[schema_name]\n",
    "    expected_cols = [c[\"name\"] for c in schema[\"columns\"]]\n",
    "\n",
    "    result = {\n",
    "        \"missing_columns\": [c for c in expected_cols if c not in df.columns],\n",
    "        \"extra_columns\": [c for c in df.columns if c not in expected_cols],\n",
    "        \"n_rows\": len(df),\n",
    "        \"n_cols\": df.shape[1]\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e2c6f6-54ae-457e-9b84-f121b7445d40",
   "metadata": {},
   "source": [
    "## Ajustar la app de datos sint√©ticos para usar el modelo local\n",
    "Llama a:\n",
    "- build_prompt\n",
    "- generate_with_local_llama\n",
    "- parse_csv_to_df\n",
    "- basic_quality_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13abf8b7-3a0f-479a-8384-7a5df072604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data_app(\n",
    "    schema_name: str,\n",
    "    n_rows: int,\n",
    "    extra_instructions: str\n",
    "):\n",
    "    prompt = build_prompt(schema_name, n_rows, extra_instructions)\n",
    "    \n",
    "    # DEBUG: ver el prompt que le mandas al modelo (una vez, en consola)\n",
    "    print(\"=== PROMPT ===\")\n",
    "    print(prompt[:1000])\n",
    "    print(\"=============\")\n",
    "\n",
    "    raw_output = generate_with_local_llama(prompt)\n",
    "\n",
    "    # DEBUG: ver lo que devuelve el modelo\n",
    "    print(\"=== RAW OUTPUT (primeros 1000 chars) ===\")\n",
    "    print(raw_output[:1000])\n",
    "    print(\"========================================\")\n",
    "\n",
    "    df = parse_csv_to_df(raw_output)\n",
    "    checks = basic_quality_checks(df, schema_name)\n",
    "\n",
    "    # DEBUG: ver tama√±o del df\n",
    "    print(\"=== DF SHAPE ===\", df.shape)\n",
    "    print(df.head())\n",
    "\n",
    "    info = (\n",
    "        f\"Filas generadas (df): {checks['n_rows']}\\n\"\n",
    "        f\"Columnas extra: {checks['extra_columns']}\\n\"\n",
    "        f\"Columnas faltantes: {checks['missing_columns']}\\n\"\n",
    "    )\n",
    "\n",
    "    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "    df.to_csv(tmp_file.name, index=False)\n",
    "    tmp_file_path = tmp_file.name\n",
    "    tmp_file.close()\n",
    "\n",
    "    return info, df, tmp_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f3689-e06a-438a-9313-d05e36aa020e",
   "metadata": {},
   "source": [
    "## Interfaz Gradio (sin selector de modelo, porque usamos Llama local)\n",
    "Construye la interfaz:\n",
    "- Dropdown para tipo de dataset.\n",
    "- Slider para n√∫mero de filas.\n",
    "- Textbox para instrucciones extra.\n",
    "- Bot√≥n que llama synthetic_data_app.\n",
    "- Muestra info, dataframe y archivo descargable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c5929c-4779-4827-9eb3-cdda51bb9318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://f2916aa809dbc758cb.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f2916aa809dbc758cb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT ===\n",
      "Eres un generador de datos sint√©ticos tabulares para pruebas de anal√≠tica y machine learning.\n",
      "Tu tarea es generar un dataset SINT√âTICO en formato CSV, sin datos personales reales.\n",
      "Dataset: Bank Transactions\n",
      "Descripci√≥n: Movimientos bancarios de cuentas de ahorro.\n",
      "\n",
      "Especificaci√≥n de columnas:\n",
      "- transaction_id (string): √∫nico\n",
      "- customer_id (string): formato CUST-XXXX\n",
      "- transaction_date (date): 2024-01-01 a 2024-12-31\n",
      "- transaction_type (category): deposit, withdrawal, transfer\n",
      "- amount (float): entre 10 y 5_000\n",
      "- balance_after (float): saldo posterior coherente\n",
      "- channel (category): ATM, web, mobile_app, branch\n",
      "\n",
      "Genera exactamente 100 filas.\n",
      "Muy importante:\n",
      "1. La salida debe estar SOLO en formato CSV.\n",
      "2. La primera fila debe ser el encabezado con los nombres de las columnas.\n",
      "3. No incluyas explicaciones, comentarios ni texto adicional.\n",
      "4. Respeta tipos y rangos lo mejor posible.\n",
      "\n",
      "Instrucciones adicionales del usuario:\n",
      "Genera un 10% de transacciones fraudulentas\n",
      "=============\n",
      "=== RAW OUTPUT (primeros 1000 chars) ===\n",
      ". Estas transacciones tendr√°n un tipo de transacci√≥n de 'fraud'.\n",
      "\n",
      "transaction_id,customer_id,transaction_date,transaction_type,amount,balance_after,channel\n",
      "CUST-0001,2024-01-01,deposit,100.0,100.0,web\n",
      "CUST-0002,2024-01-02,withdrawal,50.0,50.0,ATM\n",
      "CUST-0003,2024-01-03,transfer,200.0,300.0,mobile_app\n",
      "CUST-0004,2024-01-04,deposit,500.0,800.0,branch\n",
      "CUST-0005,2024-01-05,withdrawal,250.0,550.0,web\n",
      "CUST-0006,2024-01-06,transfer,300.0,850.0,mobile_app\n",
      "CUST-0007,2024-01-07,deposit,1200.0,2050.0,branch\n",
      "CUST-0008,2024-01-08,withdrawal,400.0,1650.0,ATM\n",
      "CUST-0009,2024-01-09,transfer,600.0,2350.0,mobile_app\n",
      "CUST-0010,2024-01-10,deposit,1500.0,3850.0,web\n",
      "CUST-0011,2024-01-11,withdrawal,800.0,3050.0,ATM\n",
      "CUST-0012,2024-01-12,transfer,1000.0,4050.0,mobile_app\n",
      "CUST-0013,2024-01-13,deposit,1800.0,5850.0,branch\n",
      "CUST-0014,2024-01-14,withdrawal,300.0,5550.0,web\n",
      "CUST-0015,2024-01-15,transfer,500.0,6050.0,mobile_app\n",
      "CUST-0016,2024-01-16,deposit,2500.0,8550.0,branch\n",
      "CUST-0017,2024-01-17,withdrawal,1200.0,7350.\n",
      "========================================\n",
      "=== DF SHAPE === (73, 7)\n",
      "  transaction_id customer_id transaction_date  transaction_type  amount  \\\n",
      "0      CUST-0001  2024-01-01          deposit             100.0   100.0   \n",
      "1      CUST-0002  2024-01-02       withdrawal              50.0    50.0   \n",
      "2      CUST-0003  2024-01-03         transfer             200.0   300.0   \n",
      "3      CUST-0004  2024-01-04          deposit             500.0   800.0   \n",
      "4      CUST-0005  2024-01-05       withdrawal             250.0   550.0   \n",
      "\n",
      "  balance_after  channel  \n",
      "0           web      NaN  \n",
      "1           ATM      NaN  \n",
      "2    mobile_app      NaN  \n",
      "3        branch      NaN  \n",
      "4           web      NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_4899/698440775.py\", line 33, in synthetic_data_app\n",
      "    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
      "               ^^^^^^^^\n",
      "NameError: name 'tempfile' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_4899/698440775.py\", line 33, in synthetic_data_app\n",
      "    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
      "               ^^^^^^^^\n",
      "NameError: name 'tempfile' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/queueing.py\", line 789, in process_events\n",
      "    await run_sync(self.compute_analytics_summary, self.event_analytics)\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/queueing.py\", line 177, in compute_analytics_summary\n",
      "    df = self._get_df(event_analytics)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/LLM/lib/python3.11/site-packages/gradio/queueing.py\", line 168, in _get_df\n",
      "    .infer_objects(copy=False)  # type: ignore\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: NDFrame.infer_objects() got an unexpected keyword argument 'copy'\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(title=\"Synthetic Data Studio\") as demo:\n",
    "    gr.Markdown(\"# üß™ Synthetic Data Studio\\nGenerador de datos sint√©ticos con Llama 3.1 (4-bit)\")\n",
    "\n",
    "    schema_name = gr.Dropdown(\n",
    "        choices=list(DATASET_SCHEMAS.keys()),\n",
    "        value=\"Retail Sales\",\n",
    "        label=\"Tipo de dataset\"\n",
    "    )\n",
    "\n",
    "    n_rows = gr.Slider(10, 1000, value=100, step=10, label=\"N√∫mero de filas\")\n",
    "\n",
    "    extra_instructions = gr.Textbox(\n",
    "        lines=4,\n",
    "        label=\"Instrucciones adicionales (opcional)\",\n",
    "        placeholder=\"Ej: Genera un 10% de transacciones fraudulentas...\"\n",
    "    )\n",
    "\n",
    "    generate_btn = gr.Button(\"Generar datos sint√©ticos üöÄ\")\n",
    "\n",
    "    info_out = gr.Textbox(label=\"Informaci√≥n de generaci√≥n\")\n",
    "    df_out = gr.Dataframe(label=\"Vista previa del dataset\")\n",
    "    csv_out = gr.File(label=\"Descargar CSV\")\n",
    "\n",
    "    generate_btn.click(\n",
    "        synthetic_data_app,\n",
    "        inputs=[schema_name, n_rows, extra_instructions],\n",
    "        outputs=[info_out, df_out, csv_out]\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b221c-a2a3-420f-917b-72ee6d9564ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320c12c-2050-490c-8b17-d0daced8e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ Clean up GPU memory after finishing the project\n",
    "\n",
    "import gc\n",
    "\n",
    "# Si existieran estas variables en el espacio global, las borramos con try/except\n",
    "for var_name in [\"MODEL_LLAMA\", \"tokenizer_llama\"]:\n",
    "    try:\n",
    "        del globals()[var_name]\n",
    "        print(f\"Deleted: {var_name}\")\n",
    "    except KeyError:\n",
    "        print(f\"{var_name} not found in globals()\")\n",
    "\n",
    "# Forzamos garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Limpiar cache de CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ torch.cuda.empty_cache() called\")\n",
    "else:\n",
    "    print(\"CUDA no est√° disponible en este entorno.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5ef53-d44b-49f4-b2ba-120069de8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb72f5b-513c-45f7-a667-63f3b4ca679e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
