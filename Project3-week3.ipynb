{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a8098a",
   "metadata": {},
   "source": [
    "1. Visi√≥n del proyecto (para explicarlo en clase/entrevista)\n",
    "\n",
    "‚ÄúConstru√≠ un generador de datos sint√©ticos que usa modelos de lenguaje open-source (Llama 3.1 / 3.2 y Gemma 2) para crear datasets tabulares a partir de descripciones de negocio. El usuario define el dominio (ej. ventas, banca, salud), el tama√±o del dataset y las reglas; el modelo genera datos en formato CSV. Encima de esto constru√≠ una interfaz con Gradio para que cualquier usuario pueda usarlo sin escribir c√≥digo.‚Äù\n",
    "\n",
    "Casos de uso a mencionar:\n",
    "\n",
    "- Probar pipelines de datos cuando no hay datos reales disponibles.\n",
    "- Crear datos despersonalizados para demos / prototipos.\n",
    "- Augmentaci√≥n de datos para ejercicios de ML.\n",
    "\n",
    "2. Tecnolog√≠as a usar\n",
    "\n",
    "- Modelos LLM (texto-texto):\n",
    "- meta-llama/Llama-3.1-8B-Instruct \n",
    "- meta-llama/Llama-3.2-3B-Instruct \n",
    "- google/gemma-2-9b-it \n",
    "\n",
    "Librer√≠as Python:\n",
    "\n",
    "- transformers o huggingface_hub (para llamar al modelo).\n",
    "- pandas (para construir el DataFrame).\n",
    "\n",
    "gradio (UI).\n",
    "\n",
    "- Auth Hugging Face con token. \n",
    "\n",
    "3. Estructura sugerida del notebook Project3-week3.ipynb\n",
    "\n",
    "- T√≠tulo + descripci√≥n del proyecto (Markdown).\n",
    "- Instalaci√≥n / imports.\n",
    "- Configuraci√≥n de modelos Hugging Face.\n",
    "- Definici√≥n de esquemas de datasets (plantillas de negocio).\n",
    "- Funci√≥n generadora usando LLM.\n",
    "- Conversi√≥n a pandas.DataFrame + validaciones b√°sicas.\n",
    "- Interfaz Gradio.\n",
    "\n",
    "Pruebas y ejemplos de uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89022227",
   "metadata": {},
   "source": [
    "## Imports e instalaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub gradio pandas bitsandbytes dotenv openai torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb487d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148d20b",
   "metadata": {},
   "source": [
    "## Login y carga de variables de entorno\n",
    "- Carga .env.\n",
    "- Toma OPENAI_API_KEY y HUGGINGFACE_API_KEY.\n",
    "- Crea un cliente de OpenAI si hay API key.\n",
    "- Llama a login() de Hugging Face si hay token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(dotenv_path='/workspace/.env', override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "hf_token = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "if openai_api_key:\n",
    "    openai_client = OpenAI(api_key=openai_api_key)\n",
    "    print(\" OpenAI Client Initialized\")\n",
    "else:\n",
    "    print(\" OpenAI API Key not found\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "if hf_token:\n",
    "    if hf_token.startswith('Bearer '):\n",
    "        hf_token = hf_token.replace('Bearer ', '')\n",
    "    login(hf_token.strip())\n",
    "    print(\" Logged into Hugging Face\")\n",
    "else:\n",
    "    print(\" Hugging Face Token not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3dcee",
   "metadata": {},
   "source": [
    "## Configuraci√≥n de modelos\n",
    "- Definision de nombres de modelos en Hugging Face.\n",
    "- En este proyecto se est√° usando LLAMA_3_1 en 4 bits; los dem√°s est√°n listos por si se usan luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model identifiers\n",
    "LLAMA_3_1 = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "LLAMA_3_2 = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "PHI4 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "GEMMA3 = \"google/gemma-3-4b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864bcff",
   "metadata": {},
   "source": [
    "## Cache Management\n",
    "\n",
    "To keep our workspace organized, we will define specific directories for each model. This prevents models from filling up the default cache partition and allows for easier management.\n",
    "\n",
    "- Define ruta base de cach√© HF.\n",
    "- Crea subdirectorios para cada modelo.\n",
    "- Imprime rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Hugging Face cache directory\n",
    "hf_cache_base = os.getenv('HF_HOME', '/root/.cache/huggingface')\n",
    "\n",
    "# Define specific cache directories for each model\n",
    "model_cache_llama_3_1_8b = os.path.join(hf_cache_base, 'models', 'llama_3_1_8b')\n",
    "model_cache_llama_3_2_3b = os.path.join(hf_cache_base, 'models', 'llama_3_2_3b')\n",
    "model_cache_phi = os.path.join(hf_cache_base, 'models', 'phi_3_mini')\n",
    "model_cache_gemma = os.path.join(hf_cache_base, 'models', 'gemma_3_4b')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(model_cache_llama_3_1_8b, exist_ok=True)\n",
    "os.makedirs(model_cache_llama_3_2_3b, exist_ok=True)\n",
    "os.makedirs(model_cache_phi, exist_ok=True)\n",
    "os.makedirs(model_cache_gemma, exist_ok=True)\n",
    "\n",
    "print(f\"Llama Cache: {model_cache_llama_3_1_8b}\")\n",
    "print(f\"Llama Cache: {model_cache_llama_3_2_3b}\")\n",
    "print(f\"Phi-3 Cache: {model_cache_phi}\")\n",
    "print(f\"Gemma Cache: {model_cache_gemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d54ddf-f51c-41a5-8567-5cf411bd1eb9",
   "metadata": {},
   "source": [
    "## Definir ‚Äúplantillas de datasets‚Äù\n",
    "3 dominios de negocio:\n",
    "- \"Retail Sales\"\n",
    "- \"Bank Transactions\"\n",
    "- \"Customer Support Tickets\"\n",
    "\n",
    "Cada uno tiene:\n",
    "\n",
    "- description\n",
    "- columns: lista de columnas con name, type, constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eed930-5c55-45af-b549-b7713488255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SCHEMAS = {\n",
    "    \"Retail Sales\": {\n",
    "        \"description\": \"Ventas en una tienda retail de e-commerce.\",\n",
    "        \"columns\": [\n",
    "            {\"name\": \"order_id\", \"type\": \"string\", \"constraints\": \"√∫nico, formato ORD-XXXX\"},\n",
    "            {\"name\": \"order_date\", \"type\": \"date\", \"constraints\": \"entre 2024-01-01 y 2024-12-31\"},\n",
    "            {\"name\": \"customer_id\", \"type\": \"string\", \"constraints\": \"formato CUST-XXXX\"},\n",
    "            {\"name\": \"country\", \"type\": \"category\", \"constraints\": \"Colombia, M√©xico, Chile, Per√∫\"},\n",
    "            {\"name\": \"product_category\", \"type\": \"category\", \"constraints\": \"Electr√≥nicos, Ropa, Hogar\"},\n",
    "            {\"name\": \"unit_price\", \"type\": \"float\", \"constraints\": \"entre 5 y 200\"},\n",
    "            {\"name\": \"quantity\", \"type\": \"int\", \"constraints\": \"entre 1 y 10\"},\n",
    "            {\"name\": \"total_amount\", \"type\": \"float\", \"constraints\": \"unit_price * quantity\"},\n",
    "            {\"name\": \"is_fraud\", \"type\": \"bool\", \"constraints\": \"True si la transacci√≥n es fraudulenta, False en caso contrario\"}\n",
    "        ]\n",
    "    },\n",
    "    \"Bank Transactions\": {\n",
    "        \"description\": \"Movimientos bancarios de cuentas de ahorro.\",\n",
    "        \"columns\": [\n",
    "            {\"name\": \"transaction_id\", \"type\": \"string\", \"constraints\": \"√∫nico\"},\n",
    "            {\"name\": \"customer_id\", \"type\": \"string\", \"constraints\": \"formato CUST-XXXX\"},\n",
    "            {\"name\": \"transaction_date\", \"type\": \"date\", \"constraints\": \"2024-01-01 a 2024-12-31\"},\n",
    "            {\"name\": \"transaction_type\", \"type\": \"category\", \"constraints\": \"deposit, withdrawal, transfer\"},\n",
    "            {\"name\": \"amount\", \"type\": \"float\", \"constraints\": \"entre 10 y 5_000\"},\n",
    "            {\"name\": \"balance_after\", \"type\": \"float\", \"constraints\": \"saldo posterior coherente\"},\n",
    "            {\"name\": \"channel\", \"type\": \"category\", \"constraints\": \"ATM, web, mobile_app, branch\"}\n",
    "        ]\n",
    "    },\n",
    "    \"Customer Support Tickets\": {\n",
    "        \"description\": \"Tickets de soporte para una plataforma SaaS.\",\n",
    "        \"columns\": [\n",
    "            {\"name\": \"ticket_id\", \"type\": \"string\", \"constraints\": \"√∫nico\"},\n",
    "            {\"name\": \"created_at\", \"type\": \"datetime\", \"constraints\": \"2024-01-01 a 2024-12-31\"},\n",
    "            {\"name\": \"customer_tier\", \"type\": \"category\", \"constraints\": \"Free, Standard, Premium\"},\n",
    "            {\"name\": \"issue_type\", \"type\": \"category\", \"constraints\": \"bug, billing, onboarding, other\"},\n",
    "            {\"name\": \"priority\", \"type\": \"category\", \"constraints\": \"low, medium, high, critical\"},\n",
    "            {\"name\": \"resolution_time_hours\", \"type\": \"float\", \"constraints\": \">= 0\"},\n",
    "            {\"name\": \"resolved\", \"type\": \"bool\", \"constraints\": \"True/False\"}\n",
    "        ]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a130b31-1136-46ff-8023-3a415363cf60",
   "metadata": {},
   "source": [
    "## Construir el prompt para el LLM\n",
    "### Construye el prompt en espa√±ol explic√°ndole al LLM:\n",
    "- Rol (generador de datos sint√©ticos).\n",
    "- Dataset y descripci√≥n.\n",
    "- Columnas y restricciones.\n",
    "- N√∫mero de filas.\n",
    "- Reglas de salida: solo CSV, encabezado en primera fila, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033b1dc-7718-4386-9fd6-68f6b1bca73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(schema_name: str, n_rows: int, extra_instructions: str = \"\") -> str:\n",
    "    schema = DATASET_SCHEMAS[schema_name]\n",
    "    lines = []\n",
    "\n",
    "    lines.append(\n",
    "        \"Eres un generador de datos sint√©ticos tabulares para pruebas de anal√≠tica y machine learning.\"\n",
    "    )\n",
    "    lines.append(\n",
    "        \"Tu tarea es generar un dataset SINT√âTICO en formato CSV, sin datos personales reales.\"\n",
    "    )\n",
    "    lines.append(f\"Dataset: {schema_name}\")\n",
    "    lines.append(f\"Descripci√≥n: {schema['description']}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Especificaci√≥n de columnas:\")\n",
    "\n",
    "    for col in schema[\"columns\"]:\n",
    "        lines.append(\n",
    "            f\"- {col['name']} ({col['type']}): {col['constraints']}\"\n",
    "        )\n",
    "\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"Genera exactamente {n_rows} filas de datos. Es obligatorio tener {n_rows} filas.\")\n",
    "    lines.append(\"No escribas texto adicional antes ni despu√©s del CSV. Solo el CSV.\")\n",
    "    lines.append(\"Muy importante:\")\n",
    "    lines.append(\"1. La salida debe estar SOLO en formato CSV.\")\n",
    "    lines.append(\"2. La primera fila debe ser el encabezado con los nombres de las columnas.\")\n",
    "    lines.append(\"3. No incluyas explicaciones, comentarios ni texto adicional.\")\n",
    "    lines.append(\"4. Respeta tipos y rangos lo mejor posible.\")\n",
    "    if extra_instructions:\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"Instrucciones adicionales del usuario:\")\n",
    "        lines.append(extra_instructions)\n",
    "\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5daa7b-602d-483c-b648-d7f67000a8ae",
   "metadata": {},
   "source": [
    "## Cargar el tokenizer de Llama 3.1\n",
    "- Descarga / carga el tokenizer de Llama 3.1 8B.\n",
    "### Ajusta:\n",
    "- pad_token = eos_token\n",
    "- padding_side = \"left\" (√∫til para modelos causales).\n",
    "- Aqu√≠ se define tokenizer_llama, que luego usa generate_with_local_llama.\n",
    "- Si esta celda no se ejecuta antes de pulsar el bot√≥n de Gradio ‚Üí NameError: tokenizer_llama is not defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba62c42-ff6a-4f99-9e79-3c0199b00977",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_llama = AutoTokenizer.from_pretrained(\n",
    "    LLAMA_3_1,\n",
    "    cache_dir=model_cache_llama_3_1_8b\n",
    ")\n",
    "\n",
    "# Ajustes recomendados para modelos causales\n",
    "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
    "tokenizer_llama.padding_side = \"left\"\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39476b",
   "metadata": {},
   "source": [
    "## Configura  --> Quantization\n",
    "### Configura BitsAndBytes para cargar el modelo:\n",
    "- En 4 bits (load_in_4bit=True)\n",
    "- NF4 como tipo de cuantizaci√≥n.\n",
    "- bfloat16 como tipo de c√≥mputo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181ad96-30f0-45be-a8a8-9ef07bbba70b",
   "metadata": {},
   "source": [
    "### Carga del modelo Llama 3.1 en 4-bit\n",
    "- Carga el modelo en 4-bit usando quant_config, mapeando dispositivos autom√°ticamente.\n",
    "- Usa el mismo cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0b04a-1bc4-4757-9893-66e72d45afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LLAMA = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_3_1, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=quant_config,\n",
    "    cache_dir=model_cache_llama_3_1_8b\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully from: {model_cache_llama_3_1_8b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e6a493-e66c-459a-b097-8ac035e1fb16",
   "metadata": {},
   "source": [
    "## Funci√≥n generate_with_local_llama con limpieza de tensores\n",
    "- Usa tokenizer_llama y MODEL_LLAMA\n",
    "- Genera texto con sampling (temperature, top_p).\n",
    "- Limpia memoria: del inputs, generated_ids + torch.cuda.empty_cache()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67db43-d549-474e-a935-001373ca293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_local_llama(prompt: str, max_new_tokens: int = 4096) -> str:\n",
    "    # Tokenizamos el prompt\n",
    "    inputs = tokenizer_llama(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(MODEL_LLAMA.device)\n",
    "\n",
    "    # Generaci√≥n\n",
    "    with torch.no_grad():\n",
    "        generated_ids = MODEL_LLAMA.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_llama.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decodificar el texto completo (prompt + respuesta)\n",
    "    full_text = tokenizer_llama.decode(\n",
    "        generated_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # Limpiar tensores intermedios para liberar memoria GPU\n",
    "    del inputs, generated_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Opcional: quedarte solo con lo generado despu√©s del prompt\n",
    "    generated_part = full_text[len(prompt):].strip()\n",
    "    return generated_part if generated_part else full_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333920c9-b787-477f-9a53-752ecbcf61ba",
   "metadata": {},
   "source": [
    "### Parsear la salida a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5478fc3-ec3b-4fde-b8ab-9b85c8a91826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def parse_csv_to_df(text: str) -> pd.DataFrame:\n",
    "    # Quitar posibles bloques markdown ```csv ... ```\n",
    "    cleaned = re.sub(r\"```(?:csv)?\", \"\", text)\n",
    "    cleaned = cleaned.strip(\"` \\n\")\n",
    "\n",
    "    # Nos quedamos solo con las l√≠neas que contengan comas (probable CSV)\n",
    "    lines = [l for l in cleaned.splitlines() if \",\" in l]\n",
    "    if not lines:\n",
    "        print(\"WARN: No se encontraron l√≠neas con comas en la salida del modelo.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    csv_text = \"\\n\".join(lines)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(io.StringIO(csv_text))\n",
    "    except Exception as e:\n",
    "        print(\"Error al parsear CSV:\", e)\n",
    "        print(\"Contenido que se intent√≥ parsear:\")\n",
    "        print(csv_text[:500])\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d2272-e647-4702-884f-721bc2abcafe",
   "metadata": {},
   "source": [
    "### Validaciones b√°sicas\n",
    "- ¬øN√∫mero de filas = solicitado?\n",
    "- ¬øHay columnas faltantes?\n",
    "- ¬øTipos b√°sicos (int, float) se pueden convertir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a849db-02fe-4b1f-a696-ea19cf070d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_quality_checks(df: pd.DataFrame, schema_name: str) -> dict:\n",
    "    schema = DATASET_SCHEMAS[schema_name]\n",
    "    expected_cols = [c[\"name\"] for c in schema[\"columns\"]]\n",
    "\n",
    "    result = {\n",
    "        \"missing_columns\": [c for c in expected_cols if c not in df.columns],\n",
    "        \"extra_columns\": [c for c in df.columns if c not in expected_cols],\n",
    "        \"n_rows\": len(df),\n",
    "        \"n_cols\": df.shape[1]\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e2c6f6-54ae-457e-9b84-f121b7445d40",
   "metadata": {},
   "source": [
    "## Ajustar la app de datos sint√©ticos para usar el modelo local\n",
    "Llama a:\n",
    "- build_prompt\n",
    "- generate_with_local_llama\n",
    "- parse_csv_to_df\n",
    "- basic_quality_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abf8b7-3a0f-479a-8384-7a5df072604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data_app(\n",
    "    schema_name: str,\n",
    "    n_rows: int,\n",
    "    extra_instructions: str\n",
    "):\n",
    "    prompt = build_prompt(schema_name, n_rows, extra_instructions)\n",
    "    \n",
    "    # DEBUG: ver el prompt que le mandas al modelo (una vez, en consola)\n",
    "    print(\"=== PROMPT ===\")\n",
    "    print(prompt[:1000])\n",
    "    print(\"=============\")\n",
    "\n",
    "    raw_output = generate_with_local_llama(prompt)\n",
    "\n",
    "    # DEBUG: ver lo que devuelve el modelo\n",
    "    print(\"=== RAW OUTPUT (primeros 1000 chars) ===\")\n",
    "    print(raw_output[:1000])\n",
    "    print(\"========================================\")\n",
    "\n",
    "    df = parse_csv_to_df(raw_output)\n",
    "    checks = basic_quality_checks(df, schema_name)\n",
    "\n",
    "    # DEBUG: ver tama√±o del df\n",
    "    print(\"=== DF SHAPE ===\", df.shape)\n",
    "    print(df.head())\n",
    "\n",
    "    info = (\n",
    "        f\"Filas generadas (df): {checks['n_rows']}\\n\"\n",
    "        f\"Columnas extra: {checks['extra_columns']}\\n\"\n",
    "        f\"Columnas faltantes: {checks['missing_columns']}\\n\"\n",
    "    )\n",
    "\n",
    "    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "    df.to_csv(tmp_file.name, index=False)\n",
    "    tmp_file_path = tmp_file.name\n",
    "    tmp_file.close()\n",
    "\n",
    "    return info, df, tmp_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f3689-e06a-438a-9313-d05e36aa020e",
   "metadata": {},
   "source": [
    "## Interfaz Gradio (sin selector de modelo, porque usamos Llama local)\n",
    "Construye la interfaz:\n",
    "- Dropdown para tipo de dataset.\n",
    "- Slider para n√∫mero de filas.\n",
    "- Textbox para instrucciones extra.\n",
    "- Bot√≥n que llama synthetic_data_app.\n",
    "- Muestra info, dataframe y archivo descargable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5929c-4779-4827-9eb3-cdda51bb9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Synthetic Data Studio\") as demo:\n",
    "    gr.Markdown(\"# üß™ Synthetic Data Studio\\nGenerador de datos sint√©ticos con Llama 3.1 (4-bit)\")\n",
    "\n",
    "    schema_name = gr.Dropdown(\n",
    "        choices=list(DATASET_SCHEMAS.keys()),\n",
    "        value=\"Retail Sales\",\n",
    "        label=\"Tipo de dataset\"\n",
    "    )\n",
    "\n",
    "    n_rows = gr.Slider(10, 1000, value=100, step=10, label=\"N√∫mero de filas\")\n",
    "\n",
    "    extra_instructions = gr.Textbox(\n",
    "        lines=4,\n",
    "        label=\"Instrucciones adicionales (opcional)\",\n",
    "        placeholder=\"Ej: Genera un 10% de transacciones fraudulentas...\"\n",
    "    )\n",
    "\n",
    "    generate_btn = gr.Button(\"Generar datos sint√©ticos üöÄ\")\n",
    "\n",
    "    info_out = gr.Textbox(label=\"Informaci√≥n de generaci√≥n\")\n",
    "    df_out = gr.Dataframe(label=\"Vista previa del dataset\")\n",
    "    csv_out = gr.File(label=\"Descargar CSV\")\n",
    "\n",
    "    generate_btn.click(\n",
    "        synthetic_data_app,\n",
    "        inputs=[schema_name, n_rows, extra_instructions],\n",
    "        outputs=[info_out, df_out, csv_out]\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b221c-a2a3-420f-917b-72ee6d9564ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320c12c-2050-490c-8b17-d0daced8e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ Clean up GPU memory after finishing the project\n",
    "\n",
    "import gc\n",
    "\n",
    "# Si existieran estas variables en el espacio global, las borramos con try/except\n",
    "for var_name in [\"MODEL_LLAMA\", \"tokenizer_llama\"]:\n",
    "    try:\n",
    "        del globals()[var_name]\n",
    "        print(f\"Deleted: {var_name}\")\n",
    "    except KeyError:\n",
    "        print(f\"{var_name} not found in globals()\")\n",
    "\n",
    "# Forzamos garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Limpiar cache de CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ torch.cuda.empty_cache() called\")\n",
    "else:\n",
    "    print(\"CUDA no est√° disponible en este entorno.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5ef53-d44b-49f4-b2ba-120069de8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb72f5b-513c-45f7-a667-63f3b4ca679e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
