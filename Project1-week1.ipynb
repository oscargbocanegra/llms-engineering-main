{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Project 1: Personalized Programming Tutor\n",
                "\n",
                "## Executive Summary\n",
                "\n",
                "This project demonstrates the creation of an **AI-powered educational tool** designed to explain complex programming concepts in a personalized, easy-to-understand manner. \n",
                "\n",
                "By leveraging Large Language Models (LLMs), this application acts as a virtual tutor that:\n",
                "1.  **Analyzes** raw code snippets provided by the user.\n",
                "2.  **Deconstructs** the logic into step-by-step explanations.\n",
                "3.  **Teaches** the underlying concepts and best practices.\n",
                "4.  **Delivers** content using real-time streaming for an engaging user experience.\n",
                "\n",
                "**Key Engineering Concepts:**\n",
                "- **LLM Integration**: Seamless switching between OpenAI (Cloud) and Ollama (Local) providers.\n",
                "- **Prompt Engineering**: Designing robust system prompts to enforce a specific teaching persona.\n",
                "- **Streaming Responses**: Implementing token-by-token rendering for low-latency feedback.\n",
                "- **Interactive UI**: Building a command-line interface (CLI) for user interaction."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports_desc",
            "metadata": {},
            "source": [
                "## 1. Environment Setup & Imports\n",
                "\n",
                "We begin by importing the necessary libraries. \n",
                "- `os` and `dotenv`: For secure management of API keys and configuration via environment variables.\n",
                "- `IPython.display`: To render Markdown and handle real-time display updates in the notebook.\n",
                "- `openai`: The standard client library used to interact with both OpenAI's API and Ollama's OpenAI-compatible endpoint."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "from dotenv import load_dotenv\n",
                "from IPython.display import Markdown, display, update_display\n",
                "from openai import OpenAI"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "config_desc",
            "metadata": {},
            "source": [
                "## 2. Provider Configuration\n",
                "\n",
                "This system is designed to be **model-agnostic**. You can switch between a cloud provider (OpenAI) for maximum capability or a local provider (Ollama) for privacy and zero cost.\n",
                "\n",
                "Change the `USE_PROVIDER` variable below to switch backends."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# CONFIGURATION: Choose your LLM provider\n",
                "# Options: 'ollama' (Local) or 'openai' (Cloud)\n",
                "USE_PROVIDER = 'ollama' \n",
                "\n",
                "print(f\"Selected provider: {USE_PROVIDER.upper()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "init_desc",
            "metadata": {},
            "source": [
                "## 3. Client Initialization\n",
                "\n",
                "Here we initialize the API client based on the selected provider. \n",
                "\n",
                "- **Ollama**: Connects to a local server (usually `localhost:11434`). It mimics the OpenAI API structure, allowing us to use the same client code.\n",
                "- **OpenAI**: Connects to the official API using your secret key.\n",
                "\n",
                "This abstraction layer allows the rest of the application to remain unchanged regardless of the underlying model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_client",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load environment variables from .env file\n",
                "load_dotenv(dotenv_path='/workspace/.env', override=True)\n",
                "\n",
                "# Determine provider from code or environment\n",
                "llm_provider = USE_PROVIDER if 'USE_PROVIDER' in globals() else os.getenv('LLM_PROVIDER', 'ollama')\n",
                "\n",
                "if llm_provider == 'ollama':\n",
                "    # --- OLLAMA CONFIGURATION (Local) ---\n",
                "    ollama_base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
                "    ollama_api_key = os.getenv('OLLAMA_API_KEY', 'ollama')\n",
                "    ollama_model = 'qwen3-coder:480b-cloud' # Example high-performance local model\n",
                "    \n",
                "    print(\"=\" * 50)\n",
                "    print(\"üöÄ Using OLLAMA (Local)\")\n",
                "    print(f\"   Base URL: {ollama_base_url}\")\n",
                "    print(f\"   Model: {ollama_model}\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    # Initialize OpenAI client pointing to local Ollama instance\n",
                "    client = OpenAI(\n",
                "        base_url=f\"{ollama_base_url}/v1\",\n",
                "        api_key=ollama_api_key\n",
                "    )\n",
                "    MODEL = ollama_model\n",
                "    \n",
                "else:\n",
                "    # --- OPENAI CONFIGURATION (Cloud) ---\n",
                "    api_key = os.getenv('OPENAI_API_KEY')\n",
                "    \n",
                "    print(\"=\" * 50)\n",
                "    print(\"‚òÅÔ∏è Using OPENAI (Cloud API)\")\n",
                "    print(f\"   Model: gpt-4o-mini\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    client = OpenAI(api_key=api_key)\n",
                "    MODEL = 'gpt-4o-mini'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "func_desc",
            "metadata": {},
            "source": [
                "## 4. The Inference Engine\n",
                "\n",
                "This function, `callModel`, is the core of our application. It handles the communication with the LLM.\n",
                "\n",
                "**Key Feature: Streaming**\n",
                "When `use_stream=True`, the function yields chunks of text as they are generated. We use `IPython.display.update_display` to append these chunks to the screen in real-time, creating a \"typewriter\" effect. This significantly improves perceived performance, as the user doesn't have to wait for the entire explanation to be generated."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "call_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "def callModel(messages, use_stream=False):\n",
                "    \"\"\"\n",
                "    Sends messages to the LLM and handles the response.\n",
                "    \n",
                "    Args:\n",
                "        messages (list): List of message dictionaries (role, content).\n",
                "        use_stream (bool): Whether to stream the response in real-time.\n",
                "    \"\"\"\n",
                "    if use_stream:\n",
                "        stream = client.chat.completions.create(\n",
                "            model=MODEL,\n",
                "            messages=messages,\n",
                "            stream=True\n",
                "        )\n",
                "        \n",
                "        response_text = \"\"\n",
                "        display_handle = None\n",
                "        \n",
                "        for chunk in stream:\n",
                "            content = chunk.choices[0].delta.content or ''\n",
                "            response_text += content\n",
                "            \n",
                "            # Update the display dynamically\n",
                "            if display_handle is None and response_text:\n",
                "                display_handle = display(Markdown(response_text), display_id=True)\n",
                "            elif display_handle is not None:\n",
                "                update_display(Markdown(response_text), display_id=display_handle.display_id)\n",
                "        \n",
                "    else:\n",
                "        # Standard non-streaming call\n",
                "        response = client.chat.completions.create(\n",
                "            model=MODEL,\n",
                "            messages=messages\n",
                "        )\n",
                "        result = response.choices[0].message.content\n",
                "        display(Markdown(result))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "system_prompt_desc",
            "metadata": {},
            "source": [
                "## 5. Persona Design (System Prompt)\n",
                "\n",
                "The **System Prompt** is where we define the AI's behavior. For this project, we are crafting a \"Personalized Tutor\" persona.\n",
                "\n",
                "We explicitly instruct the model to:\n",
                "- Avoid overly technical jargon without explanation.\n",
                "- Use analogies to make abstract concepts concrete.\n",
                "- Focus on the \"why\" and \"how\", not just the syntax.\n",
                "- Format the output cleanly with Markdown."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "system_prompt",
            "metadata": {},
            "outputs": [],
            "source": [
                "system_prompt = \"\"\"\n",
                "You are an expert programming tutor with deep knowledge in Python, JavaScript, and software engineering.\n",
                "\n",
                "Your teaching style:\n",
                "- Explain concepts clearly and step by step.\n",
                "- Use analogies and real-world examples to make concepts stick.\n",
                "- Break down complex code into understandable parts.\n",
                "- Highlight best practices and common pitfalls.\n",
                "- Encourage learning by explaining the \"why\" behind the code.\n",
                "\n",
                "Response format:\n",
                "- Always respond in well-formatted Markdown.\n",
                "- Use headers, lists, and code blocks appropriately.\n",
                "- Do NOT wrap your entire response in markdown code fences (```).\n",
                "- Be concise but thorough.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "input_desc",
            "metadata": {},
            "source": [
                "## 6. User Interaction\n",
                "\n",
                "We create a simple interactive prompt to accept code snippets from the user. This simulates a real-world scenario where a student asks for help with a specific block of code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "user_input",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"üéì PERSONALIZED PROGRAMMING TUTOR\")\n",
                "print(\"=\" * 60)\n",
                "print(\"\\nEnter the code you want to understand.\")\n",
                "print(\"\\nExample:\")\n",
                "print('  yield from {book.get(\"author\") for book in books if book.get(\"author\")}')\n",
                "print(\"\\nYour question (paste your code below):\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "question = input()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prompt_eng_desc",
            "metadata": {},
            "source": [
                "## 7. Prompt Engineering\n",
                "\n",
                "We wrap the user's raw input into a structured prompt. This technique ensures the model knows exactly *what* to do with the code (explain it) rather than just executing it or refactoring it.\n",
                "\n",
                "We specifically ask for:\n",
                "1. Functionality summary.\n",
                "2. Step-by-step breakdown.\n",
                "3. Use cases.\n",
                "4. Key concepts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "prompt_eng",
            "metadata": {},
            "outputs": [],
            "source": [
                "user_prompt = f\"\"\"\n",
                "Please explain the following code in detail:\n",
                "\n",
                "```python\n",
                "{question}\n",
                "```\n",
                "\n",
                "Include:\n",
                "1. What this code does\n",
                "2. How it works step by step\n",
                "3. Why someone would use this approach\n",
                "4. Any important concepts or patterns involved\n",
                "\"\"\"\n",
                "\n",
                "messages = [\n",
                "    {\"role\": \"system\", \"content\": system_prompt},\n",
                "    {\"role\": \"user\", \"content\": user_prompt}\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exec_desc",
            "metadata": {},
            "source": [
                "## 8. Execution\n",
                "\n",
                "Finally, we call the model with our constructed messages and enable streaming to see the tutor in action."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exec",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Execute the tutor with the question\n",
                "callModel(messages, use_stream=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "conclusion",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "You have successfully built a **Personalized Programming Tutor**. \n",
                "\n",
                "This project demonstrates how to:\n",
                "- Configure and use different LLM providers.\n",
                "- Design effective system prompts to create specific AI personas.\n",
                "- Implement streaming for better user experience.\n",
                "- Structure a clean, professional AI application.\n",
                "\n",
                "**Future Improvements:**\n",
                "- Add a \"Follow-up Question\" feature to make it a conversation.\n",
                "- Integrate syntax highlighting for the code blocks in the output.\n",
                "- Add support for uploading code files directly."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}