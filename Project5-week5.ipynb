{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "# Project 5: chatbot con conocimiento privado\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "Crea un trabajador del conocimiento en tus datos para impulsar la productividad\n",
    "- Reune los archivos necesarios en un solo lugar, la base de conociminetos personal\n",
    "- vectoriza todo en chroma, el almacen de vectores propio\n",
    "- crea un asistente de IA conversacional y formula preguntas\n",
    "\n",
    "**Core Innovation:**\n",
    "\n",
    "**Key Capabilities:**\n",
    "\n",
    "**Business Value:**\n",
    "\n",
    "**Technical Stack:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os, shutil\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/LLM/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# imports for langchain, plotly and Chroma\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3916c7a-5c20-4447-9f5c-6db0a5d40405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB_DIR: /workspace/giova/llms-engineering-main/knowledge-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs (páginas) cargadas: 56\n",
      "Ejemplo metadata: {'producer': 'Acrobat Distiller 9.4.0 (Windows)', 'creator': 'Epic Editor v. 5.0', 'creationdate': '2011-12-14T14:40:39+00:00', 'author': 'SPSS Inc.', 'moddate': '2011-12-14T14:40:39+00:00', 'title': 'Manual CRISP-DM de IBM SPSS Modeler', 'source': '/workspace/giova/llms-engineering-main/knowledge-base/crisdm.pdf', 'total_pages': 56, 'page': 0, 'page_label': '1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Cargar PDFs (desde knowledge-base/)\n",
    "\n",
    "KB_DIR = Path(\"knowledge-base\").resolve()\n",
    "print(\"KB_DIR:\", KB_DIR)\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    str(KB_DIR),\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(\"Docs (páginas) cargadas:\", len(documents))\n",
    "print(\"Ejemplo metadata:\", documents[0].metadata if documents else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23da34a4-f922-4d69-a917-6951b12db2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 55\n",
      "Snippet chunk: i\n",
      "Manual CRISP-DM de IBM SPSS\n",
      "Modeler\n"
     ]
    }
   ],
   "source": [
    "# 2) Chunking\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"Chunks:\", len(chunks))\n",
    "print(\"Snippet chunk:\", chunks[0].page_content[:200] if chunks else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "437daef5-fd6c-4039-89b7-c2bcfa01018b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 55 documents\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
    "# Then replace embeddings = OpenAIEmbeddings()\n",
    "# with:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea62dda-0f9d-4831-be41-8eac9807f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 55 vectors with 1,536 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "403524b4-89d1-457c-9bca-6e9656298394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Batería de preguntas (ejecución automática + fuentes)\n",
    "\n",
    "test_questions = [\n",
    "    \"¿Qué significa CRISP-DM y para qué se usa?\",\n",
    "    \"¿Cuántas fases tiene CRISP-DM?\",\n",
    "    \"Enumera las 6 fases de CRISP-DM en orden.\",\n",
    "    \"¿Qué se hace en la fase de Comprensión del negocio?\",\n",
    "    \"¿Qué se hace en la fase de Comprensión de los datos?\",\n",
    "    \"¿Qué tareas típicas se mencionan en Preparación de datos?\",\n",
    "    \"¿Por qué CRISP-DM se considera un proceso iterativo?\",\n",
    "    \"Dame un resumen de 'Comprensión del negocio' y cita páginas.\",\n",
    "    \"Explica qué dice el documento sobre riesgos/contingencias y cita páginas.\",\n",
    "    \"Ok, si mi objetivo es reducir abandono de clientes, ¿qué fase hago primero y por qué?\",\n",
    "    \"Ahora reescribe ese plan en 6 bullets alineados a las fases CRISP-DM.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d45462-a818-441c-b010-b85b32bcf618",
   "metadata": {},
   "source": [
    "## Visualizing the Vector Store\n",
    "\n",
    "Let's take a minute to look at the documents and their embedding vectors to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b98adf5e-d464-4bd2-9bdf-bc5b6770263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "\n",
    "# doc_type seguro (fallback)\n",
    "doc_types = [m.get(\"doc_type\", \"unknown\") for m in metadatas]\n",
    "\n",
    "# Genera un color por cada tipo distinto (sin asumir nombres)\n",
    "unique_types = sorted(set(doc_types))\n",
    "palette = [\"blue\", \"green\", \"red\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
    "\n",
    "type_to_color = {t: palette[i % len(palette)] for i, t in enumerate(unique_types)}\n",
    "colors = [type_to_color[t] for t in doc_types]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df0ef488-01ab-49ff-bc9c-30e9b14eff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.renderers.default = \"iframe\"  # el más robusto en JupyterLab remoto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "427149d5-e5d8-4abd-bb6f-7ef0333cca21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"920px\"\n",
       "    height=\"670\"\n",
       "    src=\"iframe_figures/figure_15.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Asegura que vectors sea 2D numpy array ---\n",
    "vectors = np.array(vectors)\n",
    "assert vectors.ndim == 2, f\"vectors debe ser 2D, llegó: {vectors.shape}\"\n",
    "\n",
    "n = vectors.shape[0]\n",
    "if n < 2:\n",
    "    raise ValueError(f\"No hay suficientes embeddings para visualizar (n={n}).\")\n",
    "\n",
    "# --- t-SNE: ajusta perplexity según cantidad de puntos ---\n",
    "# Regla: perplexity < n, típicamente <= (n-1)/3\n",
    "perplexity = min(30, max(2, (n - 1) // 3))\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, init=\"pca\", learning_rate=\"auto\")\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# --- Hover text para PDF: fuente + página + tipo + snippet ---\n",
    "def safe_snippet(text, k=200):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    return (text[:k] + \"...\") if len(text) > k else text\n",
    "\n",
    "hover_text = []\n",
    "for md, t, d in zip(metadatas, doc_types, documents):\n",
    "    src = os.path.basename(md.get(\"source\", \"unknown\"))\n",
    "    page = md.get(\"page\", md.get(\"page_number\", \"\"))\n",
    "    page_str = f\"{page}\" if page != \"\" else \"?\"\n",
    "    hover_text.append(\n",
    "        f\"Type: {t}\"\n",
    "        f\"<br>Source: {src}\"\n",
    "        f\"<br>Page: {page_str}\"\n",
    "        f\"<br>Text: {safe_snippet(d)}\"\n",
    "    )\n",
    "\n",
    "# --- Plot 2D ---\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter(\n",
    "        x=reduced_vectors[:, 0],\n",
    "        y=reduced_vectors[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=6, color=colors, opacity=0.85),\n",
    "        text=hover_text,\n",
    "        hoverinfo=\"text\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"2D Chroma Vector Store (t-SNE, n={n}, perplexity={perplexity})\",\n",
    "    xaxis_title=\"t-SNE x\",\n",
    "    yaxis_title=\"t-SNE y\",\n",
    "    width=900,\n",
    "    height=650,\n",
    "    margin=dict(r=20, b=20, l=20, t=60)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1418e88-acd5-460a-bf2b-4e6efc88e3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"920px\"\n",
       "    height=\"720\"\n",
       "    src=\"iframe_figures/figure_16.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pio.renderers.default = \"iframe\"  # para JupyterLab remoto\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "n = vectors.shape[0]\n",
    "assert n == 55, f\"Esperaba 55, llegó {n}\"\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=3,\n",
    "    random_state=42,\n",
    "    perplexity=15,      # recomendado para n=55\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    ")\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='t-SNE x', yaxis_title='t-SNE y', zaxis_title='t-SNE z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "## Time to use LangChain to bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3251/1344145089.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.7)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Alternative - if you'd like to use Ollama locally, uncomment this line instead\n",
    "# llm = ChatOpenAI(temperature=0.7, model_name='llama3.2', base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968e7bf2-e862-4679-a11f-6c1efb6ec8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En la fase de Comprensión del negocio, se realizan varias actividades clave para establecer un marco claro para el proyecto de minería de datos. Estas actividades incluyen:\n",
      "\n",
      "1. **Definir objetivos comerciales**: Se identifican y documentan los objetivos específicos que la organización espera alcanzar con la minería de datos. Esto ayuda a alinear los esfuerzos del proyecto con las metas empresariales.\n",
      "\n",
      "2. **Valorar la situación actual**: Se evalúa la situación comercial existente, incluyendo la disponibilidad de datos, recursos humanos, problemas existentes y factores de riesgo. Esto permite tener una visión clara de los recursos y limitaciones del proyecto.\n",
      "\n",
      "3. **Identificar áreas problemáticas**: Se determina el área específica que necesita atención, como marketing, atención al cliente o desarrollo comercial, y se describe el problema general que se busca resolver.\n",
      "\n",
      "4. **Compilación de información**: Se recopila información sobre la empresa, como la estructura organizativa, los recursos disponibles y los individuos clave que pueden influir en el proyecto.\n",
      "\n",
      "5. **Establecimiento de un plan de proyecto**: Se utiliza la información recopilada para crear un plan de proyecto que incluya todas las fases de CRISP-DM y considere riesgos y dependencias.\n",
      "\n",
      "6. **Implicar a las partes interesadas**: Se busca la participación de diversas personas dentro de la organización para asegurar que todos estén alineados y de acuerdo con los objetivos y expectativas del proyecto.\n",
      "\n",
      "En resumen, la fase de Comprensión del negocio se centra en establecer una base sólida para el proyecto de minería de datos al definir objetivos claros, valorar la situación actual y asegurar el apoyo de las partes interesadas.\n"
     ]
    }
   ],
   "source": [
    "# Let's try a simple question\n",
    "\n",
    "query = \"¿Qué se hace en la fase de Comprensión del negocio?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b5a9013-d5d4-4e25-9e7c-cdbb4f33e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "## Now we will bring this up in Gradio using the Chat interface -\n",
    "\n",
    "A quick and easy way to prototype a chat with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping that in a function\n",
    "\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://a32eeddc47c7c6bf9b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a32eeddc47c7c6bf9b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5513c3-36cc-446c-98c1-c1eb2a4743f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
