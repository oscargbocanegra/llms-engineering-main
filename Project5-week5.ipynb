{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "# Project 5: Private Knowledge Chatbot (RAG)\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This learning project builds a **private knowledge chatbot** using a classic **Retrieval-Augmented Generation (RAG)** pipeline.\n",
    "You ingest a local knowledge base (PDFs), convert it into embeddings, store it in a vector database (Chroma), and then answer questions by retrieving the most relevant chunks and grounding the response on them.\n",
    "\n",
    "### What this notebook does (end-to-end)\n",
    "- Loads PDF documents from `knowledge-base/`\n",
    "- Splits documents into overlapping text chunks (chunking)\n",
    "- Creates vector embeddings for each chunk (OpenAI embeddings by default)\n",
    "- Persists the embeddings into a local Chroma vector store (`vector_db/`)\n",
    "- Builds a conversational retrieval chain with memory (LangChain)\n",
    "- Prototypes an interactive chat UI with Gradio\n",
    "- Visualizes the embedding space with t-SNE (2D and 3D) to sanity-check the vector store\n",
    "\n",
    "### Why it matters (business framing)\n",
    "Teams often have knowledge scattered across files and folders. RAG turns those documents into a searchable “semantic index”, enabling fast Q&A and better onboarding while keeping data local to the workspace.\n",
    "\n",
    "### Key capabilities\n",
    "- **Private knowledge ingestion:** local PDFs become searchable context\n",
    "- **Semantic retrieval:** top-$k$ chunks are fetched per question\n",
    "- **Conversational memory:** follow-up questions work better with chat history\n",
    "- **Rapid prototyping:** a working chat UI in a few lines with Gradio\n",
    "- **Observability:** lightweight embedding visualization for debugging\n",
    "\n",
    "### Technical stack\n",
    "- Python, LangChain, Chroma, OpenAI (embeddings + chat), scikit-learn (t-SNE), Plotly, Gradio\n",
    "\n",
    "### Learning-oriented notes (what to pay attention to)\n",
    "- Chunk size/overlap trade-offs (recall vs cost)\n",
    "- Persistence and reset of the vector store (repeatable experiments)\n",
    "- Retrieval parameters (`k`) and how they affect answer quality\n",
    "- Limitations: this is a prototype (no formal evaluation, no strict citation formatting wired by default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff8e2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step-by-step roadmap (learning focus)\n",
    "\n",
    "Run the notebook top-to-bottom. Each step below explains *why it exists*, what it produces, and what to watch for.\n",
    "\n",
    "- **Step 0 — Imports & configuration:** bring in the libraries used for ingestion, vector storage, visualization, and UI.\n",
    "- **Step 1 — Environment setup:** load API keys and pick the model + vector DB folder.\n",
    "- **Step 2 — Document loading:** read PDFs from the local knowledge base folder.\n",
    "- **Step 3 — Chunking:** split documents into chunks suitable for embedding + retrieval.\n",
    "- **Step 4 — Vector store:** embed chunks and persist them to Chroma.\n",
    "- **Step 5 — Sanity checks:** confirm vector counts/dimensions.\n",
    "- **Step 6 — (Optional) Test questions:** keep a small question set for repeatable checks.\n",
    "- **Step 7 — Visualization:** use t-SNE plots to understand the embedding space.\n",
    "- **Step 8 — RAG chat chain:** connect retriever + LLM + memory.\n",
    "- **Step 9 — Quick query:** validate the chain produces reasonable answers.\n",
    "- **Step 10 — Gradio UI:** launch an interactive chat experience.\n",
    "\n",
    "> This notebook intentionally keeps the code minimal and functional. Changes are focused on clarity, not new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d22d04",
   "metadata": {},
   "source": [
    "## Step 0 — Imports\n",
    "\n",
    "In a learning project, it helps to know *why each dependency exists*:\n",
    "- **LangChain + Chroma**: ingestion, splitting, embeddings, retrieval, and the conversational chain\n",
    "- **Plotly + scikit-learn (t-SNE)**: visualization to sanity-check the vector store\n",
    "- **Gradio**: quick UI to interact with the chatbot\n",
    "\n",
    "> If you get import errors, install missing packages in your notebook environment and re-run Step 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0) Imports (utilities + UI + visualization)\n",
    "\n",
    "import os, shutil\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/LLM/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 0b) Imports for LangChain + Chroma + embeddings + visualization\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778fd0b9",
   "metadata": {},
   "source": [
    "## Step 1 — Configuration and credentials\n",
    "\n",
    "This step defines (1) which chat model to use and (2) where the local vector database will be persisted.\n",
    "\n",
    "**Inputs**\n",
    "- `OPENAI_API_KEY` from your `.env` (or your environment variables)\n",
    "\n",
    "**Outputs**\n",
    "- A configured environment that downstream steps can use to embed text and call the chat model\n",
    "\n",
    "**What to watch for**\n",
    "- If `OPENAI_API_KEY` is not set, embedding or chat calls will fail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1) Environment setup (.env + API keys)\n",
    "\n",
    "# This repo commonly stores the .env file at /workspace/.env when running in containers.\n",
    "# We keep a safe fallback to the local .env (current working directory).\n",
    "dotenv_path = os.getenv(\"DOTENV_PATH\", \"/workspace/.env\")\n",
    "load_dotenv(dotenv_path=dotenv_path, override=True)\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Ensure the OpenAI SDK sees the key (set it in your .env for real usage).\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"your-key-if-not-using-env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f214a",
   "metadata": {},
   "source": [
    "## Step 2 — Load documents (private knowledge base)\n",
    "\n",
    "We load PDFs from the local `knowledge-base/` folder. Each PDF is treated as a set of pages. In RAG, pages (or page chunks) become the raw material for retrieval.\n",
    "\n",
    "**Output**\n",
    "- A list of `Document` objects (typically one per page), including metadata like source file name and page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3916c7a-5c20-4447-9f5c-6db0a5d40405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB_DIR: /workspace/giova/llms-engineering-main/knowledge-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs (páginas) cargadas: 56\n",
      "Ejemplo metadata: {'producer': 'Acrobat Distiller 9.4.0 (Windows)', 'creator': 'Epic Editor v. 5.0', 'creationdate': '2011-12-14T14:40:39+00:00', 'author': 'SPSS Inc.', 'moddate': '2011-12-14T14:40:39+00:00', 'title': 'Manual CRISP-DM de IBM SPSS Modeler', 'source': '/workspace/giova/llms-engineering-main/knowledge-base/crisdm.pdf', 'total_pages': 56, 'page': 0, 'page_label': '1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2) Load PDFs (from knowledge-base/)\n",
    "\n",
    "KB_DIR = Path(\"knowledge-base\").resolve()\n",
    "print(\"KB_DIR:\", KB_DIR)\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    str(KB_DIR),\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(\"Loaded pages:\", len(documents))\n",
    "print(\"Example metadata:\", documents[0].metadata if documents else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0833f45",
   "metadata": {},
   "source": [
    "## Step 3 — Chunking strategy\n",
    "\n",
    "LLMs and embedding models work best with smaller passages. Chunking splits pages into overlapping windows so retrieval can surface the most relevant context.\n",
    "\n",
    "**Key idea**: overlap helps preserve context across chunk boundaries, improving recall for questions that span sentences/sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da34a4-f922-4d69-a917-6951b12db2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 55\n",
      "Snippet chunk: i\n",
      "Manual CRISP-DM de IBM SPSS\n",
      "Modeler\n"
     ]
    }
   ],
   "source": [
    "# Step 3) Chunking (split pages into overlapping text chunks)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"Chunks:\", len(chunks))\n",
    "print(\"Chunk snippet:\", chunks[0].page_content[:200] if chunks else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0310ff",
   "metadata": {},
   "source": [
    "## Step 4 — Embeddings + vector store (Chroma)\n",
    "\n",
    "In this step we convert each chunk into a numeric embedding vector and persist everything in Chroma. This is what enables semantic search: questions and chunks live in the same vector space.\n",
    "\n",
    "**Outputs**\n",
    "- A persisted Chroma vector store in `vector_db/`\n",
    "- An internal collection with `count()` vectors (one per chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437daef5-fd6c-4039-89b7-c2bcfa01018b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 55 documents\n"
     ]
    }
   ],
   "source": [
    "# Step 4) Build the vector store (embeddings + persistence)\n",
    "# - Each chunk becomes an embedding vector\n",
    "# - Chroma persists the collection locally (SQLite under the hood)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Optional: use free Hugging Face sentence-transformer embeddings instead of OpenAI\n",
    "# Example:\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Reset the persisted collection to keep experiments repeatable\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create and persist vector store\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea62dda-0f9d-4831-be41-8eac9807f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 55 vectors with 1,536 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "# Step 5) Sanity check: vector count + embedding dimensions\n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fefbcc",
   "metadata": {},
   "source": [
    "## Step 5 — Quick sanity checks\n",
    "\n",
    "Before building the chat layer, confirm the vector store looks reasonable: number of vectors and embedding dimensionality.\n",
    "\n",
    "- Too few vectors usually means document loading/chunking failed.\n",
    "- Unexpected dimensions can indicate a different embedding model than you intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403524b4-89d1-457c-9bca-6e9656298394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6) Test question set (repeatable checks + grounding)\n",
    "\n",
    "test_questions = [\n",
    "    \"What does CRISP-DM mean and what is it used for?\",\n",
    "    \"How many phases does CRISP-DM have?\",\n",
    "    \"List the 6 CRISP-DM phases in order.\",\n",
    "    \"What happens in the Business Understanding phase?\",\n",
    "    \"What happens in the Data Understanding phase?\",\n",
    "    \"What typical tasks are mentioned in Data Preparation?\",\n",
    "    \"Why is CRISP-DM considered an iterative process?\",\n",
    "    \"Give me a summary of 'Business Understanding' and cite pages.\",\n",
    "    \"Explain what the document says about risks/contingencies and cite pages.\",\n",
    "    \"If my goal is to reduce customer churn, which phase should I do first and why?\",\n",
    "    \"Rewrite that plan as 6 bullets aligned to the CRISP-DM phases.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d36bc",
   "metadata": {},
   "source": [
    "## Step 6 — A tiny evaluation loop (optional)\n",
    "\n",
    "This list is a lightweight way to keep your checks repeatable while iterating on chunking and retrieval settings.\n",
    "\n",
    "In a production system you would add a real evaluation harness (golden answers, citations, and scoring). For this learning project, a small question set is enough to spot obvious regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d45462-a818-441c-b010-b85b32bcf618",
   "metadata": {},
   "source": [
    "## Step 7 — Visualizing the vector store (debugging intuition)\n",
    "\n",
    "This is not required for RAG to work, but it is extremely helpful when you are learning.\n",
    "Visualization helps you build intuition about:\n",
    "- Whether embeddings look “clustered” vs random noise\n",
    "- Whether document types (if present in metadata) separate in space\n",
    "- Whether there are obvious outliers (often caused by very short/empty chunks)\n",
    "\n",
    "We use t-SNE (2D and 3D) as a quick exploratory tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98adf5e-d464-4bd2-9bdf-bc5b6770263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "\n",
    "# Safe doc_type extraction (fallback to 'unknown')\n",
    "doc_types = [m.get(\"doc_type\", \"unknown\") for m in metadatas]\n",
    "\n",
    "# Assign one color per distinct type (without assuming known names)\n",
    "unique_types = sorted(set(doc_types))\n",
    "palette = [\"blue\", \"green\", \"red\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
    "\n",
    "type_to_color = {t: palette[i % len(palette)] for i, t in enumerate(unique_types)}\n",
    "colors = [type_to_color[t] for t in doc_types]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ef488-01ab-49ff-bc9c-30e9b14eff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.renderers.default = \"iframe\"  # most robust option for remote Jupyter/JupyterLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427149d5-e5d8-4abd-bb6f-7ef0333cca21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"920px\"\n",
       "    height=\"670\"\n",
       "    src=\"iframe_figures/figure_15.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Ensure vectors is a 2D NumPy array ---\n",
    "vectors = np.array(vectors)\n",
    "assert vectors.ndim == 2, f\"vectors must be 2D, got shape: {vectors.shape}\"\n",
    "\n",
    "n = vectors.shape[0]\n",
    "if n < 2:\n",
    "    raise ValueError(f\"Not enough embeddings to visualize (n={n}).\")\n",
    "\n",
    "# --- t-SNE: adapt perplexity to the number of points ---\n",
    "# Rule of thumb: perplexity < n, typically <= (n-1)/3\n",
    "perplexity = min(30, max(2, (n - 1) // 3))\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, init=\"pca\", learning_rate=\"auto\")\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# --- Hover text for PDFs: source + page + type + snippet ---\n",
    "def safe_snippet(text, k=200):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    return (text[:k] + \"...\") if len(text) > k else text\n",
    "\n",
    "hover_text = []\n",
    "for md, t, d in zip(metadatas, doc_types, documents):\n",
    "    src = os.path.basename(md.get(\"source\", \"unknown\"))\n",
    "    page = md.get(\"page\", md.get(\"page_number\", \"\"))\n",
    "    page_str = f\"{page}\" if page != \"\" else \"?\"\n",
    "    hover_text.append(\n",
    "        f\"Type: {t}\"\n",
    "        f\"<br>Source: {src}\"\n",
    "        f\"<br>Page: {page_str}\"\n",
    "        f\"<br>Text: {safe_snippet(d)}\"\n",
    "    )\n",
    "\n",
    "# --- Plot 2D ---\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter(\n",
    "        x=reduced_vectors[:, 0],\n",
    "        y=reduced_vectors[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=6, color=colors, opacity=0.85),\n",
    "        text=hover_text,\n",
    "        hoverinfo=\"text\"\n",
    "    )]\n",
    " )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"2D Chroma Vector Store (t-SNE, n={n}, perplexity={perplexity})\",\n",
    "    xaxis_title=\"t-SNE x\",\n",
    "    yaxis_title=\"t-SNE y\",\n",
    "    width=900,\n",
    "    height=650,\n",
    "    margin=dict(r=20, b=20, l=20, t=60)\n",
    " )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1418e88-acd5-460a-bf2b-4e6efc88e3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"920px\"\n",
       "    height=\"720\"\n",
       "    src=\"iframe_figures/figure_16.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pio.renderers.default = \"iframe\"  # most robust option for remote Jupyter/JupyterLab\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "n = vectors.shape[0]\n",
    "assert n == 55, f\"Expected 55 vectors, got {n}\"\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=3,\n",
    "    random_state=42,\n",
    "    perplexity=15,      # tuned for n=55\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    ")\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text',\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='t-SNE x', yaxis_title='t-SNE y', zaxis_title='t-SNE z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "## Step 8 — Build the RAG chat chain (LangChain)\n",
    "\n",
    "Now we connect three pieces:\n",
    "- **Retriever** (Chroma): finds the top-$k$ most relevant chunks\n",
    "- **LLM** (Chat model): writes the final answer using retrieved context\n",
    "- **Memory**: keeps chat history so follow-ups make sense\n",
    "\n",
    "This is the core “knowledge worker” behavior: retrieve → ground → respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3251/1344145089.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 8) Create the conversational retrieval chain (LLM + retriever + memory)\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.7)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Alternative (local): if you'd like to use Ollama, you can switch the backend like this:\n",
    "# llm = ChatOpenAI(temperature=0.7, model_name='llama3.2', base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# Conversation memory stores the chat history (useful for follow-ups)\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# Build the RAG chain: retrieval + chat model + memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e7bf2-e862-4679-a11f-6c1efb6ec8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En la fase de Comprensión del negocio, se realizan varias actividades clave para establecer un marco claro para el proyecto de minería de datos. Estas actividades incluyen:\n",
      "\n",
      "1. **Definir objetivos comerciales**: Se identifican y documentan los objetivos específicos que la organización espera alcanzar con la minería de datos. Esto ayuda a alinear los esfuerzos del proyecto con las metas empresariales.\n",
      "\n",
      "2. **Valorar la situación actual**: Se evalúa la situación comercial existente, incluyendo la disponibilidad de datos, recursos humanos, problemas existentes y factores de riesgo. Esto permite tener una visión clara de los recursos y limitaciones del proyecto.\n",
      "\n",
      "3. **Identificar áreas problemáticas**: Se determina el área específica que necesita atención, como marketing, atención al cliente o desarrollo comercial, y se describe el problema general que se busca resolver.\n",
      "\n",
      "4. **Compilación de información**: Se recopila información sobre la empresa, como la estructura organizativa, los recursos disponibles y los individuos clave que pueden influir en el proyecto.\n",
      "\n",
      "5. **Establecimiento de un plan de proyecto**: Se utiliza la información recopilada para crear un plan de proyecto que incluya todas las fases de CRISP-DM y considere riesgos y dependencias.\n",
      "\n",
      "6. **Implicar a las partes interesadas**: Se busca la participación de diversas personas dentro de la organización para asegurar que todos estén alineados y de acuerdo con los objetivos y expectativas del proyecto.\n",
      "\n",
      "En resumen, la fase de Comprensión del negocio se centra en establecer una base sólida para el proyecto de minería de datos al definir objetivos claros, valorar la situación actual y asegurar el apoyo de las partes interesadas.\n"
     ]
    }
   ],
   "source": [
    "# Quick smoke test: ask one question and inspect the answer text\n",
    "\n",
    "query = \"What happens in the Business Understanding phase?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a9013-d5d4-4e25-9e7c-cdbb4f33e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9) Reset memory (fresh chat session)\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# Rebuild the chain with fresh memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "## Step 10 — Gradio UI (interactive demo)\n",
    "\n",
    "A chat UI makes it easy to test the system like an end user would: ask questions, refine prompts, and quickly validate improvements.\n",
    "\n",
    "This is a prototype UI (learning-first): no auth, no rate limiting, and minimal logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10) Wrap the chain into a simple function for Gradio\n",
    "\n",
    "def chat(question, history):\n",
    "    # Note: Gradio provides `history`, but the LangChain memory inside `conversation_chain` is what we use here.\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://a32eeddc47c7c6bf9b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a32eeddc47c7c6bf9b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5513c3-36cc-446c-98c1-c1eb2a4743f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
