{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Project 2: FlightAI - Multimodal Airline Assistant\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This project showcases a **production-grade multimodal AI assistant** designed for the airline industry. It goes beyond simple text chat by integrating **voice**, **images**, and **structured data** into a seamless customer service experience.\n",
    "\n",
    "**Key Capabilities:**\n",
    "1.  **Multimodal Interaction**: Users can speak to the assistant (Audio-to-Text) and hear responses back (Text-to-Speech).\n",
    "2.  **Visual Context**: The system automatically generates high-quality images of destinations mentioned in the conversation.\n",
    "3.  **Autonomous Action**: The AI uses **Function Calling** to query real-time ticket prices and perform database write operations (reservations) without manual intervention.\n",
    "4.  **Polyglot Support**: Built-in translation tools allow it to serve a global audience.\n",
    "\n",
    "**Technical Stack:**\n",
    "- **LLM**: DeepSeek-V3.1 671B (via Ollama Cloud) for high-reasoning capabilities.\n",
    "- **Audio**: OpenAI Whisper (ASR) and Google TTS.\n",
    "- **Vision**: Pollinations.AI (Flux Model) for generative imagery.\n",
    "- **UI**: Gradio for a responsive, interactive web interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_desc",
   "metadata": {},
   "source": [
    "## 1. Environment & Library Setup\n",
    "\n",
    "We start by importing the core libraries that power our multimodal features:\n",
    "- `gradio`: For building the web interface.\n",
    "- `openai`: For interacting with the LLM API.\n",
    "- `gtts` & `pydub`: For text-to-speech synthesis and audio processing.\n",
    "- `whisper`: For robust speech recognition.\n",
    "- `PIL` (Pillow): For image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgr\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from gtts import gTTS\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_desc",
   "metadata": {},
   "source": [
    "## 2. Configuration & API Initialization\n",
    "\n",
    "We load sensitive credentials from the `.env` file. \n",
    "\n",
    "**Model Selection**: We are using `deepseek-v3.1:671b-cloud`. This is a massive, state-of-the-art open model hosted on Ollama's cloud, chosen for its exceptional ability to handle complex tool-calling scenarios which smaller local models might struggle with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# API configuration\n",
    "OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL')\n",
    "OLLAMA_API_KEY = os.getenv('OLLAMA_API_KEY')\n",
    "\n",
    "# Model configuration\n",
    "OLLAMA_CHAT_MODEL = \"deepseek-v3.1:671b-cloud\"\n",
    "POLLINATIONS_API_URL = \"https://image.pollinations.ai/prompt/{prompt}\"\n",
    "\n",
    "# Initialize OpenAI-compatible client\n",
    "OLLAMA_CLIENT = OpenAI(base_url=f\"{OLLAMA_BASE_URL}/v1\", api_key=OLLAMA_API_KEY)\n",
    "\n",
    "print(f\"✅ Connected to Ollama Cloud: {OLLAMA_BASE_URL}\")\n",
    "print(f\"✅ Image Service Ready: Pollinations.AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "audio_desc",
   "metadata": {},
   "source": [
    "## 3. Audio Processing (Text-to-Speech)\n",
    "\n",
    "To make the assistant feel alive, we implement a custom TTS function. \n",
    "\n",
    "**Innovation**: Standard TTS can sometimes feel too slow for conversational use. We use `pydub` to post-process the audio and speed it up by 10% (`speed=1.1`), creating a more natural, energetic speaking rhythm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tts_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio(message, speed=1.1):\n",
    "    \"\"\"\n",
    "    Generates audio from text and adjusts playback speed.\n",
    "    \n",
    "    Args:\n",
    "        message (str): Text to speak.\n",
    "        speed (float): Speed multiplier (default 1.1x).\n",
    "    \"\"\"\n",
    "    from pydub import AudioSegment\n",
    "    \n",
    "    # Generate base audio\n",
    "    tts = gTTS(text=message, lang='en', slow=False)\n",
    "    audio_fp = BytesIO()\n",
    "    tts.write_to_fp(audio_fp)\n",
    "    audio_fp.seek(0)\n",
    "    \n",
    "    # Speed up audio\n",
    "    audio = AudioSegment.from_file(audio_fp, format=\"mp3\")\n",
    "    audio_fast = audio._spawn(audio.raw_data, overrides={'frame_rate': int(audio.frame_rate * speed)})\n",
    "    audio_fast = audio_fast.set_frame_rate(audio.frame_rate)\n",
    "    \n",
    "    output = BytesIO()\n",
    "    audio_fast.export(output, format=\"mp3\")\n",
    "    return output.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "system_prompt_desc",
   "metadata": {},
   "source": [
    "## 4. Assistant Persona\n",
    "\n",
    "We define a strict system prompt. The assistant must be helpful but concise. This is crucial for voice interactions—listening to long paragraphs of text is tedious for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "system_prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant for an airline called FlightAI.\n",
    "Provide brief and courteous responses, no more than one sentence.\n",
    "Always be accurate. If you don't know the answer, say so.\n",
    "\"\"\"\n",
    "\n",
    "TTS_ENGINE = generate_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools_desc",
   "metadata": {},
   "source": [
    "## 5. Tool Definitions (Function Calling)\n",
    "\n",
    "This is the \"brain\" of the autonomous agent. We define a set of Python functions and then describe them in a JSON schema that the LLM can understand.\n",
    "\n",
    "**The Tools:**\n",
    "1.  `get_ticket_price`: A read-only tool to fetch data.\n",
    "2.  `make_reservation`: A write tool that modifies the database.\n",
    "3.  `translate_text`: A utility tool for language tasks.\n",
    "4.  `transcribe_audio`: A utility tool for processing voice input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Database\n",
    "TICKET_PRICES = {\n",
    "    \"london\": \"$799\",\n",
    "    \"paris\": \"$899\",\n",
    "    \"tokyo\": \"$1400\",\n",
    "    \"berlin\": \"$499\",\n",
    "    \"new york\": \"$650\",\n",
    "    \"barcelona\": \"$550\",\n",
    "    \"miami\": \"$450\"\n",
    "}\n",
    "\n",
    "RESERVATIONS_DB = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticket_price(destination_city):\n",
    "    \"\"\"Retrieve ticket price for destination.\"\"\"\n",
    "    city_key = destination_city.lower()\n",
    "    price = TICKET_PRICES.get(city_key, \"Unknown\")\n",
    "    return f\"The price of a ticket to {destination_city} is {price}\"\n",
    "\n",
    "def make_reservation(passenger_name, destination_city, travel_date):\n",
    "    \"\"\"Create flight reservation.\"\"\"\n",
    "    reservation_id = f\"FL{len(RESERVATIONS_DB) + 1000}\"\n",
    "    reservation = {\n",
    "        \"id\": reservation_id,\n",
    "        \"passenger\": passenger_name,\n",
    "        \"destination\": destination_city,\n",
    "        \"date\": travel_date\n",
    "    }\n",
    "    RESERVATIONS_DB.append(reservation)\n",
    "    return f\"Reservation {reservation_id} confirmed for {passenger_name} to {destination_city} on {travel_date}\"\n",
    "\n",
    "def translate_text(text, target_language):\n",
    "    \"\"\"Translate text to target language.\"\"\"\n",
    "    return f\"Translated to {target_language}: {text}\"\n",
    "\n",
    "def transcribe_audio(audio_file_path):\n",
    "    \"\"\"Convert audio to text using Whisper.\"\"\"\n",
    "    try:\n",
    "        model = whisper.load_model(\"base\")        \n",
    "        result = model.transcribe(audio_file_path)\n",
    "        return result[\"text\"]\n",
    "    except Exception as e:\n",
    "        return f\"Transcription error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool_schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Schema for the LLM\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_ticket_price\",\n",
    "            \"description\": \"Get the price of a return ticket to the destination city.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"destination_city\": {\"type\": \"string\", \"description\": \"The city that the customer wants to travel to\"}\n",
    "                },\n",
    "                \"required\": [\"destination_city\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"make_reservation\",\n",
    "            \"description\": \"Create a flight reservation for a passenger.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"passenger_name\": {\"type\": \"string\", \"description\": \"The name of the passenger\"},\n",
    "                    \"destination_city\": {\"type\": \"string\", \"description\": \"The destination city\"},\n",
    "                    \"travel_date\": {\"type\": \"string\", \"description\": \"The travel date in YYYY-MM-DD format\"}\n",
    "                },\n",
    "                \"required\": [\"passenger_name\", \"destination_city\", \"travel_date\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"translate_text\",\n",
    "            \"description\": \"Translate text to a target language.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\"type\": \"string\", \"description\": \"The text to translate\"},\n",
    "                    \"target_language\": {\"type\": \"string\", \"description\": \"Target language code (es, fr, de)\"}\n",
    "                },\n",
    "                \"required\": [\"text\", \"target_language\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"transcribe_audio\",\n",
    "            \"description\": \"Convert audio to text.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"audio_file_path\": {\"type\": \"string\", \"description\": \"Path to the audio file\"}\n",
    "                },\n",
    "                \"required\": [\"audio_file_path\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"✅ Tools configured: {len(TOOL_DEFINITIONS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "image_desc",
   "metadata": {},
   "source": [
    "## 6. Generative Image Service\n",
    "\n",
    "We integrate Pollinations.AI to generate images on the fly. This adds a visual layer to the interaction—when a user asks about Paris, they don't just get a price, they see the Eiffel Tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "image_gen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt):\n",
    "    \"\"\"Generate image using FLUX model via Pollinations.AI.\"\"\"\n",
    "    try:\n",
    "        url = POLLINATIONS_API_URL.format(prompt=requests.utils.quote(prompt))\n",
    "        response = requests.get(url, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "            return image\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Image error: {e}\")\n",
    "        return None\n",
    "\n",
    "IMAGE_GENERATOR = generate_image\n",
    "print(\"✅ Image generator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chat_logic_desc",
   "metadata": {},
   "source": [
    "## 7. The Chat Logic (The Core Loop)\n",
    "\n",
    "This is the most complex part of the system. The `chat` function handles the entire lifecycle of a request:\n",
    "\n",
    "1.  **Receive Input**: Get text from the user (or transcribed audio).\n",
    "2.  **LLM Inference**: Send the history to the LLM.\n",
    "3.  **Tool Detection**: Check if the LLM wants to call a tool.\n",
    "4.  **Tool Execution**: If yes, run the Python function and feed the result back to the LLM.\n",
    "5.  **Response Generation**: Get the final text response.\n",
    "6.  **Multimedia Enrichment**: \n",
    "    - Generate an image if a city was mentioned.\n",
    "    - Generate audio from the response text.\n",
    "7.  **Return**: Send everything back to the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exec_tool",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tool(tool_name, arguments):\n",
    "    \"\"\"Router to execute the correct tool function.\"\"\"\n",
    "    tool_map = {\n",
    "        \"get_ticket_price\": get_ticket_price,\n",
    "        \"make_reservation\": make_reservation,\n",
    "        \"translate_text\": translate_text,\n",
    "        \"transcribe_audio\": transcribe_audio\n",
    "    }\n",
    "    \n",
    "    if tool_name in tool_map:\n",
    "        return tool_map[tool_name](**arguments)\n",
    "    return f\"Unknown tool: {tool_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(history):\n",
    "    \"\"\"Main chat loop handling text, tools, images, and audio.\"\"\"\n",
    "    if not history:\n",
    "        return history, None, None\n",
    "    \n",
    "    user_message = history[-1][\"content\"]\n",
    "    \n",
    "    # 1. Build message history for API\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_MESSAGE}]\n",
    "    for msg in history[:-1]:\n",
    "        messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    image_output = None\n",
    "    audio_output = None\n",
    "    \n",
    "    try:\n",
    "        # 2. Initial LLM Call\n",
    "        response = OLLAMA_CLIENT.chat.completions.create(\n",
    "            model=OLLAMA_CHAT_MODEL,\n",
    "            messages=messages,\n",
    "            tools=TOOL_DEFINITIONS\n",
    "        )\n",
    "        \n",
    "        # 3. Handle Tool Calls Loop\n",
    "        while response.choices[0].finish_reason == \"tool_calls\":\n",
    "            assistant_message = response.choices[0].message\n",
    "            \n",
    "            # Add assistant's intent to history\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_message.content or \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": tc.id,\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": tc.function.name,\n",
    "                            \"arguments\": tc.function.arguments\n",
    "                        }\n",
    "                    }\n",
    "                    for tc in assistant_message.tool_calls\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            # Execute all requested tools\n",
    "            for tool_call in assistant_message.tool_calls:\n",
    "                tool_name = tool_call.function.name\n",
    "                tool_args = json.loads(tool_call.function.arguments)\n",
    "                tool_result = execute_tool(tool_name, tool_args)\n",
    "                \n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": tool_result,\n",
    "                    \"tool_call_id\": tool_call.id\n",
    "                })\n",
    "            \n",
    "            # Get follow-up response from LLM\n",
    "            response = OLLAMA_CLIENT.chat.completions.create(\n",
    "                model=OLLAMA_CHAT_MODEL,\n",
    "                messages=messages,\n",
    "                tools=TOOL_DEFINITIONS\n",
    "            )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        response_text = f\"I apologize, I encountered an error: {str(e)}\"\n",
    "    \n",
    "    # 4. Generate Image (Context Aware)\n",
    "    for city in TICKET_PRICES.keys():\n",
    "        if city in user_message.lower():\n",
    "            if IMAGE_GENERATOR:\n",
    "                image_prompt = f\"Beautiful vacation destination photo of {city}, high quality, professional photography\"\n",
    "                image_output = IMAGE_GENERATOR(image_prompt)\n",
    "            break\n",
    "    \n",
    "    # 5. Generate Audio Response\n",
    "    if TTS_ENGINE and response_text:\n",
    "        try:\n",
    "            audio_output = TTS_ENGINE(response_text, speed=1.1)\n",
    "        except Exception as e:\n",
    "            print(f\"TTS error: {e}\")\n",
    "    \n",
    "    # 6. Update History\n",
    "    history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "    \n",
    "    return history, image_output, audio_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ui_desc",
   "metadata": {},
   "source": [
    "## 8. User Interface (Gradio)\n",
    "\n",
    "We build a modern, responsive UI using Gradio Blocks. \n",
    "- **Chatbot**: Displays the conversation history.\n",
    "- **Image**: Shows the generated destination photos.\n",
    "- **Audio**: Plays the assistant's voice response.\n",
    "- **Microphone**: Allows for voice input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ui_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(message, history):\n",
    "    \"\"\"Helper to add user message to chat history immediately.\"\"\"\n",
    "    return \"\", history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "def process_audio(audio_file, history):\n",
    "    \"\"\"Helper to transcribe audio and add to chat history.\"\"\"\n",
    "    if audio_file is None:\n",
    "        return history\n",
    "    \n",
    "    transcription = transcribe_audio(audio_file)\n",
    "    return history + [{\"role\": \"user\", \"content\": transcription}]\n",
    "\n",
    "# Build the UI\n",
    "with gr.Blocks(title=\"FlightAI Assistant\", theme=gr.themes.Soft()) as ui:\n",
    "    gr.Markdown(\"# ✈️ FlightAI - Multimodal Airline Assistant\")\n",
    "    gr.Markdown(\"Experience the future of customer service. Ask about flights, make reservations, or just chat!\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(\n",
    "            height=500,\n",
    "            type=\"messages\",\n",
    "            label=\"Conversation\"\n",
    "        )\n",
    "        image_output = gr.Image(\n",
    "            height=500,\n",
    "            interactive=False,\n",
    "            label=\"Visual Context\"\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_output = gr.Audio(\n",
    "            autoplay=True,\n",
    "            label=\"Assistant Voice\"\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        message_input = gr.Textbox(\n",
    "            label=\"Your Message\",\n",
    "            placeholder=\"Type here or use the microphone...\",\n",
    "            scale=3\n",
    "        )\n",
    "        audio_input = gr.Audio(\n",
    "            sources=[\"microphone\"],\n",
    "            type=\"filepath\",\n",
    "            label=\"Voice Input\",\n",
    "            scale=1\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Send Message\", scale=1, variant=\"primary\")\n",
    "    \n",
    "    # Wiring up the events\n",
    "    # 1. Text Submission\n",
    "    message_input.submit(\n",
    "        add_message,\n",
    "        inputs=[message_input, chatbot],\n",
    "        outputs=[message_input, chatbot]\n",
    "    ).then(\n",
    "        chat,\n",
    "        inputs=chatbot,\n",
    "        outputs=[chatbot, image_output, audio_output]\n",
    "    )\n",
    "    \n",
    "    # 2. Button Click\n",
    "    submit_btn.click(\n",
    "        add_message,\n",
    "        inputs=[message_input, chatbot],\n",
    "        outputs=[message_input, chatbot]\n",
    "    ).then(\n",
    "        chat,\n",
    "        inputs=chatbot,\n",
    "        outputs=[chatbot, image_output, audio_output]\n",
    "    )\n",
    "    \n",
    "    # 3. Audio Recording Stop\n",
    "    audio_input.stop_recording(\n",
    "        process_audio,\n",
    "        inputs=[audio_input, chatbot],\n",
    "        outputs=chatbot\n",
    "    ).then(\n",
    "        chat,\n",
    "        inputs=chatbot,\n",
    "        outputs=[chatbot, image_output, audio_output]\n",
    "    )\n",
    "\n",
    "print(\"✅ UI constructed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "launch_desc",
   "metadata": {},
   "source": [
    "## 9. Launch Application\n",
    "\n",
    "We launch the server. Setting `share=True` creates a temporary public URL, allowing you to share this assistant with anyone in the world for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to shut down the server when finished\n",
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project demonstrates the power of **Agentic AI**. By combining a strong reasoning model (DeepSeek) with functional tools and multimedia capabilities, we created an assistant that can:\n",
    "- **See** (via image generation context).\n",
    "- **Hear** (via Whisper).\n",
    "- **Speak** (via TTS).\n",
    "- **Act** (via Function Calling).\n",
    "\n",
    "This architecture serves as a blueprint for modern, enterprise-grade AI applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
