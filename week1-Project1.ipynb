{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# Personalized Programming Tutor - Week 1 Project\n",
    "\n",
    "## Project Objective\n",
    "\n",
    "Build a **personalized tutor** that takes a technical question about code and responds with a detailed and educational explanation.\n",
    "\n",
    "### What does this tutor do?\n",
    "\n",
    "- Explains complex code in a simple and structured way\n",
    "- Provides examples and analogies\n",
    "- Teaches the \"why\" behind code patterns\n",
    "- Responds in well-formatted Markdown\n",
    "- Supports **streaming** to see responses in real-time\n",
    "\n",
    "### Features:\n",
    "\n",
    "- **Flexible**: Switch between Ollama (local, free) and OpenAI (cloud)\n",
    "- **Interactive**: Streaming mode with typewriter effect\n",
    "- **Educational**: Designed specifically for learning\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions**: Execute the cells in order and modify the `question` variable to ask new questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from scraper import fetch_website_links, fetch_website_contents\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a79f19-e9f8-4d4c-8a7e-ce7b58d3150d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected provider: OLLAMA\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION: Change here to test different providers\n",
    "# Options: 'ollama' or 'openai'\n",
    "USE_PROVIDER = 'ollama'  # Change to 'openai' to use OpenAI\n",
    "\n",
    "print(f\"Selected provider: {USE_PROVIDER.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "529c7952-36c5-4c9e-9bfe-479cb75f7db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Using OLLAMA (Local)\n",
      "==================================================\n",
      "   Base URL: http://192.168.80.200:11434\n",
      "   Model: qwen3-coder:480b-cloud\n",
      "   Cost: FREE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize and constants\n",
    "\n",
    "# Load configuration from global .env\n",
    "load_dotenv(dotenv_path='/workspace/.env', override=True)\n",
    "\n",
    "# Use provider selected in previous cell (or from .env as fallback)\n",
    "llm_provider = USE_PROVIDER if 'USE_PROVIDER' in globals() else os.getenv('LLM_PROVIDER', 'ollama')\n",
    "\n",
    "if llm_provider == 'ollama':\n",
    "    # OLLAMA CONFIGURATION (Local)\n",
    "    ollama_base_url = os.getenv('OLLAMA_BASE_URL')\n",
    "    ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "    #ollama_model = os.getenv('OLLAMA_MODEL')\n",
    "    ollama_model = 'qwen3-coder:480b-cloud'\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Using OLLAMA (Local)\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"   Base URL: {ollama_base_url}\")\n",
    "    print(f\"   Model: {ollama_model}\")\n",
    "    print(f\"   Cost: FREE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create OpenAI client pointing to Ollama\n",
    "    openai = OpenAI(\n",
    "        base_url=f\"{ollama_base_url}/v1\",\n",
    "        api_key=ollama_api_key\n",
    "    )\n",
    "    MODEL = ollama_model\n",
    "    \n",
    "else:\n",
    "    # OPENAI CONFIGURATION (Cloud)\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Using OPENAI (Cloud API)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if api_key and api_key.startswith('sk-proj-') and len(api_key) > 50:\n",
    "        print(f\"   API Key: sk-proj-...{api_key[-8:]}\")\n",
    "        print(f\"   Model: gpt-4o-mini\")\n",
    "        print(f\"   Cost: ~$0.15 / 1M tokens input\")\n",
    "        print(\"   Status: Configured correctly\")\n",
    "    else:\n",
    "        print(\"   Status: Invalid or missing API Key\")\n",
    "        print(\"   Check the .env file\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    openai = OpenAI(api_key=api_key)\n",
    "    MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08a52f22-d661-4d45-9a78-dfba570abc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call LLM API with streaming support\n",
    "\n",
    "def callModel(ask, use_stream=False):\n",
    "    \"\"\"\n",
    "    Calls the LLM model with the provided messages\n",
    "    \n",
    "    Args:\n",
    "        ask: List of messages with format [{\"role\": \"...\", \"content\": \"...\"}]\n",
    "        use_stream: If True, displays response in real-time (typewriter effect)\n",
    "    \n",
    "    Returns:\n",
    "        The complete response content\n",
    "    \"\"\"\n",
    "    if use_stream:\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=ask,\n",
    "            stream=True  # Enable streaming\n",
    "        )\n",
    "        \n",
    "        response = \"\"\n",
    "        display_handle = None\n",
    "        \n",
    "        for chunk in stream:\n",
    "            content = chunk.choices[0].delta.content or ''\n",
    "            response += content\n",
    "            \n",
    "            if display_handle is None and response:\n",
    "                display_handle = display(Markdown(response), display_id=True)\n",
    "            elif display_handle is not None:\n",
    "                update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "        \n",
    "    else:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=ask\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - PERSONALIZED TUTOR\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert programming tutor with deep knowledge in Python, JavaScript, and software engineering.\n",
    "\n",
    "Your teaching style:\n",
    "- Explain concepts clearly and step by step\n",
    "- Use analogies and real-world examples\n",
    "- Break down complex code into understandable parts\n",
    "- Highlight best practices and common pitfalls\n",
    "- Encourage learning by explaining the \"why\" behind the code\n",
    "\n",
    "Response format:\n",
    "- Always respond in well-formatted Markdown\n",
    "- Use headers, lists, and code blocks appropriately\n",
    "- Do NOT wrap your entire response in markdown code fences (```)\n",
    "- Include examples when helpful\n",
    "- Be concise but thorough\n",
    "\n",
    "Your goal is to help students truly understand programming concepts, not just memorize syntax.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0188d485-6a9f-4c5c-85b9-d0c127532e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PERSONALIZED PROGRAMMING TUTOR\n",
      "============================================================\n",
      "\n",
      "Enter the code you want to understand.\n",
      "\n",
      "Example:\n",
      "  yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "\n",
      "Your question (paste your code below):\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for chunk in stream:             content = chunk.choices[0].delta.content or ''             response += content\n"
     ]
    }
   ],
   "source": [
    "# Get the technical question from user input\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PERSONALIZED PROGRAMMING TUTOR\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nEnter the code you want to understand.\")\n",
    "print(\"\\nExample:\")\n",
    "print('  yield from {book.get(\"author\") for book in books if book.get(\"author\")}')\n",
    "print(\"\\nYour question (paste your code below):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "question = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d041ea01-3517-428a-a68b-b62bda983631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define user prompt using the question variable\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Please explain the following code in detail:\n",
    "\n",
    "```python\n",
    "{question}\n",
    "```\n",
    "\n",
    "Include:\n",
    "1. What this code does\n",
    "2. How it works step by step\n",
    "3. Why someone would use this approach\n",
    "4. Any important concepts or patterns involved\n",
    "\"\"\"\n",
    "\n",
    "# Build messages for the LLM\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d627f33-b12c-4890-8a25-0fd20d95afae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Streaming Response Handling Code Explanation\n",
       "\n",
       "## 1. What This Code Does\n",
       "\n",
       "This code processes a **streaming response** from an AI language model (like ChatGPT). It extracts text content from each chunk of the streamed response and builds up a complete response string incrementally.\n",
       "\n",
       "**Note**: The code appears to be missing proper line breaks. Here's the corrected version:\n",
       "\n",
       "```python\n",
       "for chunk in stream:\n",
       "    content = chunk.choices[0].delta.content or ''\n",
       "    response += content\n",
       "```\n",
       "\n",
       "## 2. Step-by-Step Breakdown\n",
       "\n",
       "### Step 1: `for chunk in stream:`\n",
       "- **What**: Iterates through each data chunk in a streaming response\n",
       "- **Analogy**: Like watching a video that loads progressively - you get small pieces of data as they become available\n",
       "- **Why streaming**: Instead of waiting for the complete response, you get it piece by piece\n",
       "\n",
       "### Step 2: `chunk.choices[0].delta.content or ''`\n",
       "- **`chunk.choices[0]`**: Accesses the first (usually only) choice/response option\n",
       "- **`.delta`**: Contains the incremental change/addition to the response\n",
       "- **`.content`**: The actual text content of this incremental update\n",
       "- **`or ''`**: Handles cases where `content` might be `None` (returns empty string instead)\n",
       "\n",
       "### Step 3: `response += content`\n",
       "- **What**: Appends the new content to the existing response string\n",
       "- **Result**: Builds the complete response progressively, chunk by chunk\n",
       "\n",
       "## 3. Why Use This Approach\n",
       "\n",
       "### Real-time User Experience\n",
       "```\n",
       "Instead of: [Wait 10 seconds] → \"Hello, how can I help you today?\"\n",
       "Streaming:   \"Hello,\" → \", how\" → \" can I\" → \" help\" → \" you today?\"\n",
       "```\n",
       "\n",
       "### Memory Efficiency\n",
       "- Processes data as it arrives\n",
       "- Doesn't need to store the entire response in memory at once\n",
       "- Enables handling very long responses\n",
       "\n",
       "### Better User Engagement\n",
       "- Users see progress immediately\n",
       "- Reduces perceived waiting time\n",
       "- Enables interactive applications\n",
       "\n",
       "## 4. Important Concepts and Patterns\n",
       "\n",
       "### Streaming Pattern\n",
       "```python\n",
       "# General streaming pattern\n",
       "for data_chunk in data_source:\n",
       "    process(chunk)\n",
       "    accumulate_results()\n",
       "```\n",
       "\n",
       "### Safe Property Access\n",
       "```python\n",
       "# The `or ''` pattern prevents None-related errors\n",
       "content = potentially_none_value or ''  # Python's truthiness at work\n",
       "```\n",
       "\n",
       "### Incremental Processing\n",
       "- Common in APIs, file reading, and real-time data processing\n",
       "- Enables responsive applications\n",
       "- Reduces latency perception\n",
       "\n",
       "### Typical Usage Context\n",
       "```python\n",
       "# Complete example context\n",
       "response = \"\"  # Initialize empty response\n",
       "stream = openai.ChatCompletion.create(..., stream=True)\n",
       "\n",
       "for chunk in stream:\n",
       "    content = chunk.choices[0].delta.content or ''\n",
       "    response += content\n",
       "    print(content, end='', flush=True)  # Print in real-time\n",
       "```\n",
       "\n",
       "This approach is essential for creating responsive AI applications and demonstrates key concepts in asynchronous programming and real-time data handling."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute the tutor with the question\n",
    "\n",
    "# Option 1: Without streaming (shows complete result at the end)\n",
    "# callModel(messages)\n",
    "\n",
    "# Option 2: With streaming (typewriter effect - recommended)\n",
    "callModel(messages, use_stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a5714-f4eb-4c18-9fdf-11c3a9d09a72",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bffb1a-4618-4bc9-bf8b-d1f44886ff6f",
   "metadata": {},
   "source": [
    "## How to Use This Personalized Tutor\n",
    "\n",
    "### Quick Start Guide:\n",
    "\n",
    "1. **Setup (one time)**\n",
    "   - Execute cells 2-6 to configure the LLM provider and initialize the tutor\n",
    "\n",
    "2. **Ask your question**\n",
    "   - Execute cell 7\n",
    "   - When prompted, paste or type the code you want to understand\n",
    "   - Press Enter\n",
    "\n",
    "3. **Get the explanation**\n",
    "   - Execute cell 8 to build the prompt\n",
    "   - Execute cell 9 to see the AI tutor's explanation\n",
    "   - Use `use_stream=True` for real-time typewriter effect\n",
    "\n",
    "### Tips:\n",
    "\n",
    "- You can ask about any programming concept or code snippet\n",
    "- The tutor works best with Python, JavaScript, and general programming patterns\n",
    "- For new questions, simply re-execute cells 7, 8, and 9\n",
    "- Switch between Ollama (free, local) and OpenAI (cloud) by changing cell 3\n",
    "\n",
    "### Example Questions:\n",
    "\n",
    "```python\n",
    "# Generators and yield\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\n",
    "# List comprehensions\n",
    "[x**2 for x in range(10) if x % 2 == 0]\n",
    "\n",
    "# Decorators\n",
    "@decorator\n",
    "def function(): pass\n",
    "\n",
    "# Context managers\n",
    "with open(\"file.txt\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Lambda and functional programming\n",
    "list(map(lambda x: x * 2, [1, 2, 3]))\n",
    "\n",
    "# Async/await\n",
    "async def fetch_data():\n",
    "    await asyncio.sleep(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f0e67-f4ef-4e61-a97d-ded4b6539fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
